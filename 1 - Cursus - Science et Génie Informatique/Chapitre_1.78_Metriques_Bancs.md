# Chapitre 17 : Métriques de Performance et Bancs d\'Essai pour le Quantum-AGI

## 17.1 Introduction : \"Si vous ne pouvez le mesurer, vous ne pouvez l\'améliorer\"

### 17.1.1 Le besoin critique de mesures objectives dans un domaine sujet à l\'hyperbole

Le domaine de l\'informatique quantique, et par extension la quête de l\'intelligence artificielle générale quantique (Q-AGI), se trouve à une jonction critique. D\'une part, il captive l\'imagination des technologues, des investisseurs et du grand public avec des promesses de puissance de calcul transformationnelle, capable de remodeler des industries entières, de la pharmaceutique à la finance. Les récits médiatiques et corporatifs, séduits par l\'attrait des accélérations exponentielles par rapport aux systèmes classiques, ont alimenté un cycle d\'investissement et d\'optimisme significatif. D\'autre part, la réalité scientifique et technique est beaucoup plus nuancée. Nous opérons actuellement dans ce que le physicien théoricien John Preskill a baptisé l\'ère des ordinateurs quantiques bruités à échelle intermédiaire, ou NISQ (Noisy Intermediate-Scale Quantum). Cette ère est caractérisée par des processeurs de 50 à quelques centaines de qubits, en proie à des taux d\'erreur élevés, des temps de cohérence limités et une connectivité imparfaite, ce qui freine considérablement leur capacité à réaliser leur plein potentiel théorique.

Cette dichotomie entre la promesse et la réalité a créé un environnement propice à l\'hyperbole. La communication des avancées se concentre souvent sur des métriques uniques et faciles à appréhender, comme le nombre brut de qubits, qui, bien que non sans importance, ne brossent qu\'un tableau incomplet et souvent trompeur de la capacité réelle d\'un système. Cette focalisation sur des chiffres impressionnants mais superficiels est une conséquence directe de l\'immaturité métrologique du domaine. L\'absence d\'un cadre d\'évaluation standardisé, rigoureux et universellement accepté crée un vide qui est inévitablement comblé par des affirmations simplistes. La situation est exacerbée par une concurrence intense entre les acteurs industriels, où la tentation est grande de présenter les résultats sous le jour le plus favorable, au risque de créer une situation analogue à la loi de Goodhart : lorsqu\'une mesure devient un objectif, elle cesse d\'être une bonne mesure.

Ce manque de clarté et de rigueur dans l\'évaluation de la performance n\'est pas un simple problème de relations publiques ; il constitue un obstacle fondamental au progrès scientifique et technologique. Sans mesures objectives et reproductibles, il devient extrêmement difficile pour les chercheurs de comparer différentes architectures matérielles, pour les développeurs d\'évaluer l\'efficacité de nouveaux algorithmes, et pour les agences de financement de prendre des décisions éclairées sur l\'allocation des ressources. Le risque est de poursuivre des voies de recherche qui semblent prometteuses sur la base de métriques erronées, menant à un gaspillage de ressources et, potentiellement, à une désillusion généralisée lorsque la réalité ne correspond pas à l\'enthousiasme initial. L\'établissement d\'un cadre de mesure rigoureux n\'est donc pas une simple formalité, mais une nécessité scientifique et stratégique pour guider le domaine de la promesse spéculative vers l\'utilité démontrable.

### 17.1.2 Transition du Chapitre 16 : Pour évaluer la durabilité, il faut d\'abord pouvoir mesurer la performance

Le chapitre précédent de cette monographie a exploré les défis de la durabilité des systèmes Q-AGI, en examinant les ressources énergétiques, matérielles et computationnelles colossales qui seront probablement nécessaires pour construire et faire fonctionner des machines quantiques à grande échelle et tolérantes aux pannes. Cependant, toute discussion sur la viabilité à long terme d\'une technologie --- son coût, son empreinte écologique, sa maintenabilité --- est intrinsèquement liée à la valeur qu\'elle procure. Il serait absurde de débattre des coûts de fonctionnement d\'un moteur si l\'on ne peut pas mesurer sa puissance, son couple ou son efficacité. De même, évaluer la durabilité d\'un système Q-AGI est un exercice vide de sens si nous ne disposons pas d\'un cadre robuste pour quantifier la performance computationnelle qu\'il offre en retour.

La performance est le numérateur de l\'équation de la valeur, tandis que les ressources consommées en sont le dénominateur. Avant de pouvoir optimiser le ratio, il est impératif de pouvoir mesurer le numérateur avec précision et fiabilité. Ce chapitre jette donc les bases métrologiques indispensables à l\'évaluation non seulement de la puissance brute, mais aussi de l\'utilité pratique des systèmes Q-AGI. En établissant comment mesurer ce qu\'une machine quantique *peut faire*, nous nous donnons les moyens d\'évaluer de manière critique si ce qu\'elle *coûte* pour le faire est justifiable. La performance est la condition sine qua non de la pertinence ; la mesure de la performance est donc la condition sine qua non d\'une feuille de route crédible vers une Q-AGI durable et avantageuse.

### 17.1.3 Thèse centrale : L\'avancement crédible et reproductible vers l\'AGI quantique dépend de l\'établissement d\'une suite de bancs d\'essai standardisés et multi-niveaux qui permettent de quantifier la performance de manière holistique, de la physique des qubits à la valeur applicative

La thèse fondamentale de ce chapitre est que la complexité des ordinateurs quantiques interdit une évaluation par un seul chiffre. Le chemin vers une Q-AGI fonctionnelle n\'est pas une course monolithique, mais un défi d\'ingénierie multi-couches, où chaque niveau de la pile technologique --- du substrat physique des qubits aux compilateurs logiciels, en passant par les algorithmes de haut niveau --- présente ses propres sources d\'erreurs et ses propres goulots d\'étranglement. Les inefficacités et les erreurs à un niveau inférieur ne s\'additionnent pas simplement ; elles se propagent et s\'amplifient de manière complexe et non linéaire aux niveaux supérieurs, où une petite erreur de phase au niveau d\'une porte physique peut entraîner l\'échec complet d\'un algorithme complexe.

Par conséquent, une évaluation crédible de la performance ne peut être réalisée qu\'à travers une suite hiérarchique de métriques et de bancs d\'essai, conçue pour sonder et quantifier la performance à chaque couche critique de l\'abstraction. Une telle approche holistique est nécessaire car aucun indicateur unique ne peut capturer tous les aspects pertinents de la performance d\'un ordinateur quantique. Ce chapitre propose un tel cadre, qui va :

1. **Du niveau physique**, où des métriques comme les temps de cohérence et la fidélité des portes, caractérisées par des protocoles comme le Randomized Benchmarking, évaluent la qualité brute des composants de base.
2. **Au niveau système**, où des métriques comme le Volume Quantique et le CLOPS tentent de capturer la puissance intégrée du processeur et de sa pile de contrôle, en tenant compte des effets globaux comme la connectivité et la diaphonie.
3. **Jusqu\'au niveau applicatif**, où des suites de bancs d\'essai basées sur des algorithmes réels mesurent la capacité du système à fournir des résultats utiles pour des problèmes concrets, en se concentrant sur la qualité de la solution et le temps nécessaire pour l\'obtenir.

Ce n\'est qu\'en adoptant une telle approche multi-niveau que la communauté pourra diagnostiquer les faiblesses, mesurer les progrès de manière significative et guider l\'ingénierie des systèmes vers la réalisation d\'une véritable Q-AGI.

### 17.1.4 Aperçu de la structure du chapitre

Pour développer cette thèse, ce chapitre est structuré en quatre parties principales.

La **Partie I** entreprend une critique constructive des métriques simplistes et souvent trompeuses qui dominent le discours public. Elle déconstruira la focalisation sur le nombre de qubits, analysera les limites de la fidélité des portes isolées en présence d\'erreurs corrélées, et établira une distinction rigoureuse entre la notion de \"suprématie quantique\" et celle, beaucoup plus exigeante, d\'\"avantage quantique pratique\".

La **Partie II** constitue le cœur de notre proposition, en présentant une hiérarchie de métriques pour une évaluation holistique. Elle détaillera les protocoles de mesure et les indicateurs de performance à trois niveaux distincts : le niveau physique (temps de cohérence, RB, GST), le niveau système (Volume Quantique, CLOPS, intégration de la pile logicielle) et le niveau algorithmique (suites de benchmarks applicatifs comme QASMBench et SupermarQ).

La **Partie III** se concentrera sur les principes de conception de bancs d\'essai pertinents et robustes. Elle abordera les qualités essentielles d\'un bon benchmark, telles que la pertinence, la scalabilité et la résistance à la sur-optimisation. Cette partie soulignera également le rôle indispensable et continu de la simulation classique, à la fois comme outil de vérification et comme \"adversaire\" à battre pour prouver l\'avantage.

Enfin, la **Partie IV** se tournera vers l\'avenir en explorant les métriques et les bancs d\'essai spécifiquement conçus pour les défis uniques posés par la Q-AGI. Elle examinera comment évaluer les modèles d\'apprentissage automatique quantique au-delà de la simple précision, comment mesurer l\'efficacité des agents d\'apprentissage par renforcement quantique, et comment aborder le défi de la quantification des capacités cognitives émergentes.

Le chapitre se conclura en synthétisant le cadre proposé et en lançant un appel à une culture de transparence, de reproductibilité et d\'honnêteté intellectuelle, conditions indispensables à la maturation du domaine.

## Partie I : La Critique des Métriques Simplistes et Trompeuses

L\'un des plus grands obstacles au progrès mesurable en informatique quantique est la persistance de métriques qui, bien que faciles à communiquer, masquent la complexité de la performance et peuvent orienter les efforts de recherche et développement dans des directions sous-optimales. Cette première partie a pour but de déconstruire de manière critique trois des notions les plus répandues mais les plus problématiques : la primauté du nombre de qubits, la suffisance de la fidélité des portes isolées, et la confusion entre la démonstration de la \"suprématie\" et l\'atteinte d\'un \"avantage\" pratique.

### 17.2 Au-delà du Nombre de Qubits : La Qualité avant la Quantité

La métrique la plus fréquemment citée dans les annonces publiques et les médias est sans conteste le nombre de qubits d\'un processeur quantique. Cette focalisation est compréhensible : elle offre une analogie séduisante avec le nombre de transistors dans les processeurs classiques, qui a servi de principal indicateur de progrès pendant des décennies sous l\'égide de la loi de Moore. Cependant, cette analogie est profondément erronée et a contribué à une perception déformée de l\'état de l\'art.

#### 17.2.1 Pourquoi la \"loi de Moore\" ne s\'applique pas (encore) à l\'informatique quantique

La loi de Moore, dans son essence, décrit une observation empirique sur la densité des transistors sur une puce de silicium, qui a doublé environ tous les deux ans. Ce succès reposait sur une prémisse fondamentale : chaque transistor ajouté était, pour l\'essentiel, de qualité identique et fonctionnait avec une fiabilité extrêmement élevée. Les bits classiques sont des entités robustes et largement interchangeables. En informatique quantique, cette prémisse s\'effondre. Les qubits ne sont pas des commodités uniformes ; ce sont des systèmes physiques délicats, extrêmement sensibles à leur environnement, et leur qualité peut varier considérablement non seulement d\'une technologie à l\'autre, mais aussi d\'un qubit à l\'autre sur la même puce.

Le simple fait d\'augmenter le nombre de qubits, sans une amélioration concomitante et drastique de leur qualité, ne mène pas nécessairement à une plus grande puissance de calcul. Au contraire, cela peut être contre-productif. Plus de qubits signifie souvent une plus grande densité, ce qui peut augmenter la diaphonie (crosstalk) entre eux. Cela signifie également une complexité accrue des systèmes de contrôle et de refroidissement, ce qui peut introduire de nouvelles sources de bruit. En conséquence, l\'ajout de qubits peut en fait *augmenter* le taux d\'erreur global du système, rendant la machine plus grande mais moins capable d\'exécuter des algorithmes utiles. Un exemple frappant illustre ce point : un ordinateur quantique de 700 qubits avec une fidélité de porte à deux qubits de 98% est, pour un algorithme nécessitant de l\'ordre de

N2 portes, fonctionnellement inutile, car la probabilité d\'obtenir un résultat correct est quasi nulle. En revanche, une machine de 7 qubits avec une fidélité beaucoup plus élevée pourrait exécuter un algorithme de taille correspondante avec une probabilité de succès significative.

Ainsi, la progression en informatique quantique ne suit pas une simple courbe exponentielle basée sur une seule variable. Elle représente plutôt un défi d\'optimisation dans un espace de paramètres multidimensionnel où le nombre, la qualité (mesurée par la fidélité et la cohérence), la connectivité et la vitesse sont des axes souvent en tension les uns avec les autres. Une \"loi\" de progrès en informatique quantique, si elle devait exister, devrait être une mesure holistique qui capture l\'expansion du volume de cet espace de paramètres, et non la progression le long d\'un seul de ses axes.

#### 17.2.2 Le mythe du qubit parfait : Analyse des compromis entre nombre, qualité et connectivité

Le concept d\'un \"qubit parfait\" --- un système à deux niveaux parfaitement isolés de son environnement, contrôlable avec une précision absolue et pouvant interagir à volonté avec n\'importe quel autre qubit --- est une abstraction théorique utile, mais une impossibilité physique. Chaque plateforme matérielle existante représente un ensemble différent de compromis d\'ingénierie dans la quête de l\'approximation de cet idéal.

- **Qualité vs Vitesse :** Les qubits à ions piégés, par exemple, offrent des temps de cohérence exceptionnellement longs (minutes) et des fidélités de porte très élevées (souvent supérieures à 99.9%). Cependant, les opérations de porte, qui impliquent le déplacement physique des ions ou l\'utilisation de lasers, sont relativement lentes (microsecondes à millisecondes). À l\'inverse, les qubits supraconducteurs (transmons) permettent des vitesses de porte beaucoup plus rapides (nanosecondes), mais souffrent de temps de cohérence plus courts (microsecondes) et sont plus sensibles au bruit de leur environnement, ce qui rend les fidélités élevées plus difficiles à maintenir à grande échelle.
- **Connectivité vs Scalabilité :** La connectivité décrit quels paires de qubits peuvent directement interagir pour exécuter une porte à deux qubits. Les systèmes à ions piégés peuvent, en principe, offrir une connectivité \"tout-à-tout\", où n\'importe quel qubit peut interagir directement avec n\'importe quel autre, ce qui est extrêmement avantageux pour de nombreux algorithmes. Cependant, la mise à l\'échelle de ces systèmes au-delà de quelques dizaines d\'ions dans un seul piège est un défi majeur. Les plateformes supraconductrices, en revanche, sont plus facilement fabricables à grande échelle en utilisant des techniques de lithographie standard, mais imposent généralement une connectivité restreinte, souvent limitée aux plus proches voisins sur une grille 2D. Pour faire interagir deux qubits non adjacents, il faut insérer une série de portes SWAP, qui échangent les états des qubits intermédiaires. Chaque porte SWAP ajoute de la profondeur au circuit et, surtout, des erreurs supplémentaires, ce qui peut rapidement dégrader la performance globale de l\'algorithme.
- **Nombre vs Contrôle :** À mesure que le nombre de qubits augmente, la complexité de l\'électronique de contrôle classique et des systèmes cryogéniques nécessaires pour les faire fonctionner augmente de façon spectaculaire. Pour les qubits supraconducteurs, chaque qubit nécessite plusieurs lignes de contrôle micro-ondes, et leur acheminement sur une puce densément peuplée sans introduire de diaphonie est un défi d\'ingénierie majeur. Pour les atomes neutres, contrôler et adresser individuellement des milliers d\'atomes dans un réseau optique avec des lasers est également une tâche redoutable.

En fin de compte, la performance d\'un ordinateur quantique n\'est pas dictée par sa meilleure caractéristique, mais limitée par sa plus faible. Un système avec un grand nombre de qubits mais une faible connectivité sera pénalisée par le surcoût des portes SWAP. Un système avec une excellente fidélité mais des portes lentes sera limité par la décohérence sur les longs calculs. Un banc d\'essai véritablement informatif doit être capable de sonder ces compromis et de révéler comment un système se comporte de manière intégrée, plutôt que de simplement rapporter des chiffres records sur des paramètres isolés.

### 17.3 Les Limites de la Fidélité des Portes Isolées

Après le nombre de qubits, la fidélité des portes est la métrique de qualité la plus souvent rapportée. Elle représente la probabilité qu\'une opération quantique (une porte) produise le résultat théorique attendu. Bien qu\'essentielle, la fidélité d\'une porte mesurée en isolation est un mauvais prédicteur de la performance d\'un algorithme complexe pour une raison fondamentale : dans un système multi-qubits, les portes ne fonctionnent jamais réellement en isolation.

#### 17.3.1 Le problème des erreurs corrélées et de la diaphonie (crosstalk)

La diaphonie, ou \"crosstalk\", est l\'interaction non désirée entre différents composants d\'un processeur quantique. Elle est l\'une des sources d\'erreurs les plus pernicieuses dans les systèmes NISQ. Elle peut se manifester de plusieurs manières :

- **Diaphonie de contrôle :** Une impulsion micro-ondes destinée à manipuler un qubit cible peut \"fuir\" et affecter légèrement les qubits voisins.
- **Diaphonie de mesure :** La lecture de l\'état d\'un qubit peut perturber l\'état de ses voisins.
- **Couplage résiduel :** Même lorsqu\'ils sont censés être \"inactifs\", les qubits voisins peuvent conserver un faible couplage parasite (par exemple, une interaction ZZ résiduelle), provoquant une accumulation de déphasage non désirée.

L\'effet le plus dommageable de la diaphonie est qu\'elle introduit des **erreurs corrélées**. Au lieu que chaque qubit subisse des erreurs de manière indépendante, une erreur sur un qubit rend plus probable une erreur sur un autre. Ce phénomène brise l\'une des hypothèses les plus fondamentales et les plus pratiques des modèles de bruit simples : la localité et l\'indépendance des erreurs.

Ces corrélations sont particulièrement dévastatrices pour les codes de correction d\'erreurs quantiques (QEC). La plupart des schémas de QEC, comme le code de surface, sont conçus en supposant que les erreurs sont locales (affectant un seul ou un petit nombre de qubits voisins) et non corrélées. Des erreurs corrélées à longue distance peuvent affecter simultanément plusieurs qubits de données d\'une manière qui imite un opérateur logique, créant ainsi une erreur logique que le code est incapable de détecter et de corriger. La caractérisation et l\'atténuation de la diaphonie sont donc des domaines de recherche actifs et critiques, avec des approches allant de l\'amélioration de la conception matérielle (meilleur blindage, coupleurs accordables) à des techniques logicielles intelligentes, comme l\'ordonnancement des portes pour éviter que des opérations sensibles ne s\'exécutent simultanément sur des qubits voisins.

#### 17.3.2 Pourquoi la performance d\'un algorithme n\'est pas simplement le produit des fidélités de ses portes

L\'existence d\'erreurs corrélées et dépendantes du contexte invalide le modèle le plus simple et le plus intuitif de la performance d\'un circuit : le modèle de fidélité multiplicative. Dans ce modèle naïf, on suppose que la probabilité de succès d\'un circuit entier est simplement le produit des probabilités de succès (les fidélités) de chaque porte qui le compose. Si un circuit a G portes, chacune avec une fidélité moyenne de F, la fidélité du circuit serait FG.

Ce modèle est faux. La diaphonie signifie que la fidélité d\'une porte n\'est pas une constante intrinsèque, mais dépend de ce que font les autres qubits au même moment. La fidélité d\'une couche de portes exécutées en parallèle est souvent significativement inférieure au produit des fidélités de ces mêmes portes si elles étaient exécutées séquentiellement. L\'extrapolation de la performance à partir de mesures de composants de bas niveau est donc notoirement peu fiable.

Cette prise de conscience a des implications profondes pour le benchmarking. Elle signifie que les métriques au niveau des composants, bien qu\'indispensables pour les physiciens du matériel, sont insuffisantes pour les utilisateurs d\'algorithmes. Il est impératif de disposer de benchmarks qui évaluent le système de manière holistique, en exécutant des circuits qui exercent simultanément de nombreuses parties du processeur. Ces benchmarks au niveau du système, comme le Volume Quantique, ne mesurent pas directement les erreurs de portes individuelles, mais plutôt leur effet agrégé et intégré dans le contexte d\'un calcul réaliste. Ils capturent implicitement les effets délétères de la diaphonie, des erreurs corrélées et d\'autres imperfections au niveau du système qui seraient invisibles à une analyse purement componentielle.

### 17.4 L\'Avantage Quantique : Une Notion à Définir Rigoureusement

Le but ultime de la construction d\'ordinateurs quantiques est d\'accomplir des tâches de calcul qui sont hors de portée des machines classiques les plus puissantes. Cet objectif est souvent résumé par les termes \"suprématie quantique\" ou \"avantage quantique\". Cependant, ces termes sont fréquemment utilisés de manière interchangeable et imprécise, créant une confusion qui obscurcit la nature réelle du défi. Une définition rigoureuse est essentielle pour fixer des objectifs clairs et évaluer les progrès de manière honnête.

#### 17.4.1 La différence entre la suprématie quantique (une démonstration d\'existence) et l\'avantage quantique pratique (une utilité réelle)

Le terme \"suprématie quantique\" a été introduit par John Preskill en 2011 pour décrire le moment où un ordinateur quantique programmable effectuerait une tâche de calcul qu\'aucun ordinateur classique ne pourrait accomplir dans un délai raisonnable. Il est crucial de noter que cette définition est agnostique quant à l\'utilité de la tâche elle-même. Une démonstration de suprématie est avant tout un jalon scientifique, une preuve de principe que les ordinateurs quantiques ne sont pas simplement des versions analogiques des machines de Turing, mais qu\'ils occupent une classe de complexité de calcul potentiellement plus puissante.

L\'expérience menée par Google en 2019 sur son processeur Sycamore est l\'exemple paradigmatique d\'une revendication de suprématie. La tâche consistait à échantillonner la distribution de sortie d\'un circuit quantique aléatoire. Ce problème a été spécifiquement choisi parce qu\'il est conjecturé comme étant extrêmement difficile à simuler classiquement, mais il n\'a aucune application pratique directe connue. Il s\'agit d\'une démonstration d\'existence, pas d\'une démonstration d\'utilité.

L\'\"avantage quantique pratique\", en revanche, est un objectif beaucoup plus élevé et plus pertinent sur le plan commercial. Il se réfère au point où un ordinateur quantique peut résoudre un problème *utile et concret* --- comme la simulation d\'une molécule pour la découverte de médicaments, l\'optimisation d\'un portefeuille financier ou la factorisation d\'un grand nombre --- de manière plus rapide, plus précise ou moins coûteuse que le meilleur algorithme classique connu fonctionnant sur le meilleur matériel classique disponible. Atteindre un avantage pratique nécessite non seulement une puissance de calcul brute, mais aussi une précision suffisante pour que les résultats soient fiables. Cela impliquera très probablement l\'utilisation de la correction d\'erreurs quantiques, une exigence qui n\'est pas nécessaire pour une simple démonstration de suprématie.

Pour des raisons de sensibilité culturelle, le terme \"suprématie quantique\" a été largement remplacé dans la communauté par \"avantage quantique\". Cependant, il est essentiel de conserver la distinction conceptuelle : il y a d\'un côté la démonstration d\'une capacité de calcul brute sur un problème artificiel (l\'objectif initial de la suprématie) et de l\'autre, la fourniture d\'une solution utile à un problème du monde réel (l\'objectif de l\'avantage pratique). Les efforts de benchmarking doivent se concentrer de plus en plus sur la mesure des progrès vers ce second objectif, plus exigeant mais infiniment plus précieux.

#### 17.4.2 Le défi de la comparaison avec les meilleurs algorithmes et matériels classiques, en constante amélioration

La démonstration d\'un avantage quantique n\'est pas un événement statique que l\'on atteint une fois pour toutes. C\'est une compétition dynamique contre une \"cible mouvante\" : l\'informatique classique haute performance (HPC). L\'histoire récente de l\'informatique quantique est jalonnée de revendications d\'avantage qui ont été rapidement remises en question, non pas parce que l\'expérience quantique était erronée, mais parce que la communauté classique a répondu en développant des algorithmes de simulation plus intelligents et plus efficaces.

La revendication de Google en 2019, qui estimait à 10 000 ans le temps de calcul classique nécessaire, a été contestée par IBM, qui a fait valoir qu\'avec une utilisation optimale de l\'architecture de son supercalculateur Summit, le temps pourrait être réduit à 2.5 jours. Depuis lors, d\'autres améliorations algorithmiques ont encore réduit cet écart. De même, des expériences plus récentes démontrant une \"utilité quantique\" sur des problèmes de simulation de la matière condensée ont été rapidement égalées, voire surpassées, par de nouvelles approches classiques basées sur les réseaux de tenseurs, qui se sont avérées plus précises et pouvaient même, dans certains cas, fonctionner sur un simple ordinateur portable.

Cette relation symbiotique et contradictoire est en fait saine pour les deux domaines. Les expériences quantiques établissent des défis ambitieux qui stimulent l\'innovation dans les algorithmes de simulation classiques, tandis que les progrès de la simulation classique fixent une barre de performance de plus en plus haute que les ordinateurs quantiques doivent dépasser pour être véritablement utiles.

Cela a une implication fondamentale pour le benchmarking : une comparaison équitable ne peut pas se faire contre un algorithme classique obsolète ou sous-optimal. Pour revendiquer un avantage quantique, il faut se mesurer au *meilleur* algorithme classique connu, exécuté sur une architecture HPC de pointe. Cela signifie que la communauté de l\'informatique quantique ne peut pas travailler en vase clos. Elle doit s\'engager activement avec la communauté HPC pour s\'assurer que ses benchmarks sont rigoureux et que ses revendications d\'avantage sont robustes. Tout benchmark d\'application quantique doit être accompagné d\'un benchmark classique de référence, continuellement mis à jour pour refléter l\'état de l\'art.

## Partie II : Une Hiérarchie de Métriques pour une Évaluation Holistique

Ayant établi les lacunes des métriques simplistes, nous pouvons maintenant construire un cadre d\'évaluation plus robuste. Ce cadre est fondé sur une approche hiérarchique, reconnaissant que la performance d\'un système quantique est une propriété émergente qui dépend de la qualité et de l\'efficacité de chaque couche de la pile technologique. Nous proposons une décomposition en trois niveaux fondamentaux : le niveau physique, qui caractérise les composants de base ; le niveau système, qui évalue la performance intégrée du processeur ; et le niveau algorithmique, qui mesure l\'utilité finale pour des applications concrètes.

### 17.5 Niveau 1 : Métriques au Niveau Physique

Le niveau le plus bas de notre hiérarchie concerne la caractérisation des briques élémentaires de l\'ordinateur quantique : les qubits individuels et les opérations de porte qui les manipulent. Ces métriques sont essentielles pour les physiciens et les ingénieurs qui conçoivent et fabriquent le matériel. Elles fournissent un diagnostic détaillé de la santé et de la précision des composants fondamentaux.

#### 17.5.1 Les temps de cohérence (T1, T2) et la fidélité des opérations (portes, SPAM)

Ces quatre métriques constituent le tableau de bord de base de la qualité d\'un qubit.

- **Temps de relaxation T1 :** Il s\'agit de l\'échelle de temps caractéristique de la perte d\'énergie d\'un qubit. Plus précisément, il mesure le temps moyen qu\'il faut à un qubit dans l\'état excité ∣1⟩ pour se désintégrer spontanément vers l\'état fondamental ∣0⟩. UnT1 long est une condition nécessaire pour effectuer des calculs, car il définit la durée de vie maximale de l\'information encodée dans l\'état ∣1⟩. Il est mesuré en préparant le qubit dans l\'état ∣1⟩, en le laissant évoluer librement pendant une durée variable t, puis en mesurant la probabilité qu\'il soit encore dans l\'état ∣1⟩. La courbe de décroissance exponentielle de cette probabilité permet d\'extraire T1.
- **Temps de déphasage T2 :** Cette métrique est plus subtile mais tout aussi cruciale. Elle mesure l\'échelle de temps sur laquelle l\'information de phase quantique d\'un état de superposition est perdue. Un état de superposition, tel que(∣0⟩+∣1⟩)/2, possède une relation de phase bien définie entre ses composantes ∣0⟩ et ∣1⟩. Les fluctuations aléatoires dans l\'environnement du qubit (par exemple, des champs magnétiques fluctuants) peuvent faire évoluer cette phase de manière imprévisible, un processus appelé déphasage. Le temps T2 caractérise la perte de cette cohérence de phase. Comme la perte d\'énergie (processus T1) détruit également l\'information de phase, on a toujours la relation fondamentale T2≤2T1. Le tempsT2 est souvent la véritable limite à la durée d\'un calcul quantique cohérent. Il est mesuré par des techniques d\'interférométrie comme les expériences de Ramsey.
- **Fidélité des portes :** La fidélité d\'une porte quantique est une mesure de sa précision. Elle est définie comme la probabilité que l\'opération de porte appliquée produise l\'état de sortie idéal attendu, à partir d\'un état d\'entrée donné. Elle est généralement exprimée sous la forme F=1−ϵ, où ϵ est le taux d\'erreur de la porte. On distingue la fidélité des portes à un qubit (rotations) de celle des portes à deux qubits (comme CNOT ou CZ), ces dernières étant généralement plus difficiles à réaliser et constituant la principale source d\'erreur dans les algorithmes. Une fidélité de 99.9% signifie qu\'en moyenne, une porte sur mille échoue.
- **Fidélité SPAM (State Preparation and Measurement) :** Tout calcul quantique commence par l\'initialisation des qubits dans un état connu (généralement ∣0⟩) et se termine par la mesure de leur état final pour obtenir un résultat classique. La fidélité SPAM mesure la précision combinée de ces deux processus. Une faible fidélité SPAM signifie que même si le calcul quantique lui-même était parfait, les résultats seraient corrompus par des erreurs lors de leur initialisation ou de leur lecture. L\'amélioration de la fidélité SPAM est essentielle pour atteindre le régime de tolérance aux pannes, où le taux d\'erreur logique doit être inférieur au taux d\'erreur physique.

**Analyse critique et pertinence pour la Q-AGI :** Ces métriques physiques sont le fondement de toute performance. Elles sont indispensables pour le diagnostic et l\'amélioration du matériel. Cependant, elles ne mesurent que des propriétés locales et isolées. Elles ne disent rien sur les interactions complexes et les erreurs corrélées qui émergent dans un système multi-qubits, comme la diaphonie. Un système peut avoir d\'excellents temps de cohérence et de très bonnes fidélités de portes individuelles, mais être incapable d\'exécuter un algorithme utile en raison d\'une mauvaise connectivité ou d\'une forte diaphonie. Pour une Q-AGI, ces métriques sont des conditions nécessaires mais très loin d\'être suffisantes. Elles garantissent la qualité des briques, mais pas la solidité de l\'édifice. Le tableau suivant synthétise ces métriques physiques fondamentales.

---

  Métrique                          Description (Ce qu\'elle mesure)                                                Méthode de Mesure Typique                                  Ce qu\'elle ne mesure PAS                                            Pertinence pour la Q-AGI

  **Temps de relaxation (T1)**      L\'échelle de temps de la perte d\'énergie (décroissance de \$                  1\\rangle\$ vers \$                                        0\\rangle\$).                                                        Mesure de décroissance d\'inversion-récupération.

  **Temps de déphasage (T2)**       L\'échelle de temps de la perte de cohérence de phase dans une superposition.   Interférométrie de Ramsey, échos de spin.                  Erreurs de porte systématiques (cohérentes), diaphonie.              Critique. Définit la durée de vie maximale du calcul quantique cohérent.

  **Fidélité de porte (1Q & 2Q)**   La probabilité qu\'une porte exécute son opération idéale.                      Randomized Benchmarking (RB), Gate Set Tomography (GST).   Erreurs corrélées, diaphonie, erreurs dépendantes du contexte.       Essentielle. Détermine la profondeur de circuit maximale réalisable.

  **Fidélité SPAM**                 La probabilité combinée de préparer et de mesurer correctement un état.         Mesures répétées d\'états de base connus.                  Erreurs de calcul se produisant entre la préparation et la mesure.   Critique. Une mauvaise fidélité SPAM corrompt le résultat final de tout calcul.

---

**Table 17.1: Comparaison des Métriques Physiques Fondamentales**

#### 17.5.2 Méthodes de caractérisation : Le Randomized Benchmarking (RB) et ses variantes

Pour mesurer la fidélité des portes de manière efficace et robuste, le Randomized Benchmarking (RB) est devenu un outil standard dans la communauté.

**Définition et méthodologie :** Le RB est un protocole qui estime le taux d\'erreur *moyen* d\'un ensemble de portes quantiques, généralement le groupe de Clifford (un ensemble de portes important pour la correction d\'erreurs). La procédure est la suivante :

1. On choisit une longueur de séquence m.
2. On génère une séquence de m portes de Clifford choisies au hasard.
3. On calcule (classiquement) une porte \"inverse\" qui, si toutes les opérations étaient parfaites, annulerait l\'effet de la séquence aléatoire et ramènerait le qubit à son état initial.
4. On exécute la séquence complète (les m portes aléatoires suivies de la porte inverse) sur le qubit, initialement préparé dans un état connu (par ex., ∣0⟩).
5. On mesure le qubit. Si le résultat est ∣0⟩, la séquence est un succès.
6. On répète ce processus pour de nombreuses séquences aléatoires différentes de même longueur m, et on calcule la probabilité de succès moyenne.
7. On répète les étapes 1 à 6 pour différentes longueurs de séquence m.

La probabilité de succès décroît de manière exponentielle avec la longueur de la séquence m. En ajustant une courbe exponentielle décroissante à ces données, on peut extraire un seul paramètre, p, qui est directement lié au taux d\'erreur moyen par porte de Clifford.

**Analyse critique :**

- **Forces :** Le principal avantage du RB est sa robustesse aux erreurs de préparation d\'état et de mesure (SPAM). Comme ces erreurs ne se produisent qu\'au début et à la fin de la séquence, indépendamment de sa longueur m, elles n\'affectent pas le taux de décroissance exponentielle. Elles sont absorbées dans les paramètres d\'amplitude et de décalage de la courbe ajustée, mais pas dans le paramètre de décroissance qui nous intéresse. Le RB est également beaucoup plus scalable que la tomographie complète, car ses besoins en ressources ne croissent que polynomialement avec le nombre de qubits.
- **Faiblesses :** Le RB standard ne fournit qu\'un seul chiffre : le taux d\'erreur *moyen* sur l\'ensemble du groupe de Clifford. Il ne peut pas distinguer la performance d\'une porte de celle d\'une autre et ne donne aucune information sur la nature de l\'erreur (par exemple, une sur-rotation cohérente par rapport à une décohérence incohérente). De plus, son modèle théorique suppose que le bruit est indépendant de la porte et du temps, une hypothèse qui peut être violée en pratique, notamment en présence de diaphonie. Des variantes comme le **Interleaved RB (IRB)** ont été développées pour surmonter certaines de ces limitations. Dans l\'IRB, on insère une porte spécifique que l\'on souhaite caractériser entre chaque porte de la séquence aléatoire. La différence de taux de décroissance entre le RB standard et l\'IRB permet d\'isoler la fidélité de cette porte spécifique.

**Pertinence pour la Q-AGI :** Le RB est un outil de diagnostic de niveau physique indispensable. Il fournit une mesure de la qualité globale des opérations logiques d\'un processeur, qui est plus représentative de leur performance dans le contexte de longs algorithmes que les mesures de fidélité de portes totalement isolées. Pour une Q-AGI, qui nécessitera l\'exécution de circuits extrêmement profonds, connaître le taux d\'erreur moyen par porte est une première étape cruciale pour estimer la faisabilité d\'un algorithme et les ressources nécessaires pour la correction d\'erreurs.

#### 17.5.3 La tomographie de portes et de processus (GST/QPT) pour une caractérisation complète mais coûteuse

Pour obtenir une description complète et détaillée d\'une opération quantique, des techniques plus puissantes mais aussi plus coûteuses sont nécessaires : la tomographie.

**Définition et méthodologie :**

- **Tomographie de Processus Quantique (QPT) :** La QPT est une procédure expérimentale visant à reconstruire entièrement la \"matrice de processus\" (χ-matrice) ou la \"matrice de transfert de Pauli\" (PTM) qui décrit mathématiquement une opération quantique. Une telle matrice capture non seulement l\'opération idéale, mais aussi toutes les déviations et les processus de bruit qui l\'accompagnent. Pour ce faire, on prépare un ensemble complet d\'états d\'entrée (par exemple, les états propres des opérateurs de Pauli X, Y et Z), on applique le processus inconnu à chacun d\'eux, puis on effectue une tomographie d\'état complète sur chaque état de sortie pour le reconstruire.
- **Tomographie d\'Ensemble de Portes (GST) :** La GST est une évolution de la QPT qui résout l\'un de ses défauts majeurs. La QPT suppose que la préparation des états d\'entrée et la mesure des états de sortie sont parfaites, ce qui n\'est jamais le cas en pratique. Ces erreurs SPAM se propagent dans l\'estimation du processus et la corrompent. La GST, en revanche, est une méthode auto-cohérente qui caractérise simultanément un ensemble complet de portes *ainsi que* les opérations de préparation et de mesure. Elle y parvient en exécutant des séquences de portes beaucoup plus longues et variées, ce qui permet d\'amplifier les erreurs de manière à pouvoir les distinguer des erreurs SPAM.

**Analyse critique :**

- **Forces :** La GST est considérée comme la référence absolue (\"gold standard\") pour la caractérisation détaillée des portes. Elle fournit le modèle d\'erreur le plus complet possible, capable de distinguer les erreurs cohérentes (par exemple, une erreur d\'angle de rotation) des erreurs incohérentes (décohérence) et de quantifier précisément les erreurs SPAM. Cette richesse d\'informations est inestimable pour le débogage du matériel et l\'étalonnage précis des impulsions de contrôle.
- **Faiblesses :** Le principal inconvénient de la tomographie est son coût exorbitant en ressources. Le nombre d\'expériences nécessaires pour la QPT et la GST augmente exponentiellement avec le nombre de qubits (d4 pour un système à d dimensions). Par conséquent, en pratique, la tomographie complète n\'est réalisable que pour les portes agissant sur un très petit nombre de qubits (généralement un ou deux).

**Pertinence pour la Q-AGI :** Bien que totalement non-scalable pour caractériser un processeur Q-AGI entier, la GST joue un rôle de \"microscope\" indispensable. Elle permet de disséquer avec une précision extrême les opérations fondamentales sur un ou deux qubits, qui sont les briques de base de tous les algorithmes plus complexes. Les modèles d\'erreur détaillés obtenus par GST sont cruciaux pour alimenter des simulations de bruit précises, pour concevoir des stratégies d\'atténuation d\'erreurs au niveau des impulsions, et pour valider les hypothèses qui sous-tendent les codes de correction d\'erreurs. C\'est un outil de diagnostic profond, pas un benchmark de performance à grande échelle.

### 17.6 Niveau 2 : Métriques au Niveau Système

Une fois que les composants physiques de base ont été caractérisés, l\'étape suivante consiste à évaluer comment ils fonctionnent ensemble en tant que système intégré. Les métriques de niveau système sont conçues pour être holistiques, c\'est-à-dire qu\'elles visent à capturer la performance globale du processeur en tenant compte des interactions complexes entre les qubits, la connectivité, la diaphonie et la pile logicielle. Elles fournissent une vue d\'ensemble de la capacité de calcul utile d\'une machine.

#### 17.6.1 Le Volume Quantique (Quantum Volume) : Analyse approfondie de la méthodologie, de ses forces (holistique) et de ses faiblesses (agnostique à la vitesse et à l\'application)

Introduit par IBM, le Volume Quantique (QV) a été l\'une des premières tentatives sérieuses de créer une métrique à un seul chiffre qui va au-delà du simple nombre de qubits pour évaluer la performance globale d\'un ordinateur quantique.

**Définition et méthodologie :** Le QV mesure la taille du plus grand circuit quantique \"carré\" qu\'une machine peut exécuter avec succès. Un circuit est dit \"carré\" si sa largeur (le nombre de qubits,

w\) est égale à sa profondeur (le nombre de couches de portes, d). La procédure de mesure est la suivante :

1. On choisit une taille de circuit w=d.
2. On génère un grand nombre de circuits aléatoires de cette taille. Chaque couche du circuit est composée de permutations aléatoires des qubits suivies de portes à deux qubits (des éléments aléatoires de SU(4)) appliquées à des paires de qubits.
3. Chacun de ces circuits est exécuté de nombreuses fois sur l\'ordinateur quantique pour collecter des statistiques sur les chaînes de bits de sortie.
4. Parallèlement, les mêmes circuits sont simulés sur un ordinateur classique pour déterminer la distribution de probabilité de sortie idéale.
5. Pour chaque circuit, on identifie les \"sorties lourdes\" (heavy outputs), qui sont les chaînes de bits dont la probabilité d\'apparition dans la distribution idéale est supérieure à la médiane.
6. On calcule la \"probabilité de sortie lourde\" (heavy output probability, HOP) expérimentale, qui est la fraction des mesures qui ont abouti à une sortie lourde.
7. Un circuit de taille w est considéré comme \"réussi\" si la HOP moyenne, sur l\'ensemble des circuits aléatoires, est supérieure à 2/3 avec un niveau de confiance statistique élevé.
8. Le Volume Quantique est alors défini comme 2w, où w est la plus grande largeur de circuit pour laquelle le test est réussi.

**Analyse critique :**

- **Forces :** La principale force du QV est son caractère holistique. Pour réussir un test QV de taille w, un ordinateur doit posséder non seulement au moins w qubits, mais aussi des taux d\'erreur suffisamment faibles (pour les portes, la mesure, la diaphonie) et une connectivité assez bonne pour pouvoir exécuter un circuit de profondeur w. Une amélioration de n\'importe quel aspect du système (meilleures fidélités de portes, meilleure connectivité, compilateur plus efficace, réduction de la diaphonie) peut contribuer à augmenter le QV. Il s\'agit donc d\'une mesure de la performance *intégrée* du système.
- **Faiblesses :** Le QV présente plusieurs limitations importantes. Premièrement, il est complètement agnostique à la vitesse d\'exécution. Deux machines pourraient avoir le même QV, mais l\'une pourrait exécuter les circuits en quelques secondes et l\'autre en plusieurs heures. Deuxièmement, il repose sur l\'exécution de circuits aléatoires. Bien que ces circuits soient conçus pour être difficiles à simuler et pour sonder de manière générique l\'espace des états, leur structure n\'est pas nécessairement représentative des circuits hautement structurés que l\'on trouve dans des algorithmes importants comme l\'algorithme de Shor ou les algorithmes variationnels. Une machine pourrait être optimisée pour bien performer sur des circuits aléatoires mais mal sur des circuits structurés. Enfin, la méthodologie QV exige une simulation classique pour déterminer les distributions de probabilité idéales. Cela devient exponentiellement coûteux à mesure que le nombre de qubits augmente, ce qui signifie que le benchmark lui-même devient infaisable à vérifier pour les systèmes mêmes qu\'il est censé évaluer.

**Pertinence pour la Q-AGI :** Dans l\'ère NISQ, le QV est un baromètre utile et standardisé du progrès général des capacités matérielles. Il fournit une mesure concise de la puissance effective d\'un système. Cependant, pour la Q-AGI, son utilité est limitée. Une Q-AGI ne résoudra pas des problèmes aléatoires, mais des tâches spécifiques et structurées liées à l\'apprentissage, à l\'optimisation et au raisonnement. Le QV nous dit si une machine est globalement \"bonne\", mais pas si elle est \"bonne à quelque chose\" de particulier. Il mesure le potentiel, pas encore l\'utilité applicative.Le tableau suivant offre une analyse comparative des principales métriques système.

---

  Métrique                       Principe de Base                                                                                  Forces                                                                                            Faiblesses                                                                                                Dépendance à la Simulation Classique

  **Volume Quantique (QV)**      Taille du plus grand circuit carré aléatoire exécuté avec succès.                                 Holistique (intègre nombre de qubits, erreurs, connectivité, diaphonie). Standardisé.             Agnostique à la vitesse. Les circuits aléatoires peuvent ne pas être représentatifs.                      Oui, pour la vérification des \"sorties lourdes\". Devient infaisable pour les grands systèmes.

  **CLOPS**                      Nombre de couches de circuits de type QV exécutées par seconde.                                   Mesure directement la vitesse d\'exécution. Teste la pile de contrôle classique-quantique.        Basé sur des circuits aléatoires. La valeur dépend du QV du système, ce qui complique les comparaisons.   Non, pour la mesure de la vitesse elle-même, mais hérite des hypothèses du QV.

  **#AQ (Algorithmic Qubits)**   Nombre de qubits pouvant exécuter avec succès une suite de circuits algorithmiques spécifiques.   Basé sur des algorithmes plus réalistes que des circuits aléatoires. Vise à mesurer l\'utilité.   Spécifique à la suite d\'algorithmes choisie (peut introduire un biais). Moins standardisé que le QV.     Oui, pour calculer la fidélité par rapport aux résultats idéaux de chaque circuit.

---

**Table 17.2: Analyse Comparative des Métriques Système Holistiques**

#### 17.6.2 Les métriques de vitesse : CLOPS (Circuit Layer Operations Per Second)

Pour remédier à la principale lacune du Volume Quantique --- son indifférence à la vitesse --- IBM a introduit en 2021 la métrique CLOPS.

**Définition et méthodologie :** CLOPS est explicitement conçu pour mesurer la vitesse de traitement d\'un processeur quantique. Il est défini comme le nombre de \"couches de circuit\" qu\'un système peut exécuter par seconde. Les couches de circuit sont celles définies dans le cadre du benchmark QV. Le protocole CLOPS est exigeant : il consiste à exécuter un grand nombre (par exemple, 100) de circuits QV paramétrés. Pour chaque circuit, les paramètres sont mis à jour de manière itérative (par exemple, 10 fois), où les paramètres de l\'itération

k dépendent des résultats de mesure de l\'itération k−1. Le temps total est mesuré depuis le début de la première exécution jusqu\'à la fin de la dernière.

**Analyse critique :**

- **Forces :** CLOPS mesure la vitesse de bout en bout. Il ne chronomètre pas seulement l\'exécution des portes quantiques, mais l\'ensemble du cycle : la génération des paramètres, la compilation à la volée, le temps de communication avec le QPU, l\'exécution quantique, la mesure, le retour des résultats et le traitement classique pour préparer l\'itération suivante. Il s\'agit donc d\'une excellente mesure de la performance de l\'ensemble de la pile d\'exécution classique-quantique, ce qui est particulièrement pertinent pour les algorithmes hybrides itératifs.
- **Faiblesses :** Comme le QV, le CLOPS est basé sur des circuits aléatoires, ce qui soulève les mêmes questions sur sa pertinence pour les algorithmes structurés. De plus, la valeur CLOPS est intrinsèquement liée au QV du système testé, car la profondeur du circuit utilisé dans le calcul est le logarithme du QV. Il est donc difficile de comparer directement les scores CLOPS de deux machines ayant des QV différents. Une machine avec un QV plus faible pourrait atteindre un score CLOPS plus élevé simplement parce qu\'elle exécute des circuits plus simples. Des versions plus récentes du protocole, comme CLOPS_h, tentent de normaliser cela en se basant sur d\'autres protocoles de circuit, mais la complexité de la comparaison demeure.

**Pertinence pour la Q-AGI :** La vitesse de calcul et, plus important encore, la latence de la boucle de rétroaction, seront des facteurs absolument critiques pour de nombreuses applications de Q-AGI, notamment l\'apprentissage par renforcement (où un agent doit interagir rapidement avec son environnement) ou l\'entraînement de grands modèles quantiques. CLOPS est la première tentative sérieuse de standardiser une métrique de vitesse qui capture cette latence de la boucle hybride. Bien que le protocole spécifique puisse évoluer, le *concept* d\'une métrique comme CLOPS est essentiel pour évaluer l\'aptitude d\'une plateforme à héberger des algorithmes de Q-AGI itératifs et en temps réel.

#### 17.6.3 Vers des métriques d\'intégration : Évaluation de la performance de la pile logicielle et du temps de communication classique-quantique

La véritable performance ressentie par un utilisateur final ne dépend pas seulement du matériel quantique brut, mais de l\'ensemble de la pile informatique. Cela inclut le compilateur, qui traduit un circuit algorithmique de haut niveau en une séquence d\'opérations natives du matériel, le planificateur, qui gère l\'exécution des tâches, et l\'infrastructure de communication qui relie les processeurs classiques et quantiques.

Le goulot d\'étranglement pour de nombreux algorithmes NISQ, en particulier les algorithmes variationnels qui sont au cœur de l\'apprentissage automatique quantique, n\'est souvent pas la vitesse des portes quantiques elles-mêmes, mais la latence de la communication entre le processeur quantique (QPU) et le processeur classique (CPU) qui exécute la boucle d\'optimisation. Chaque itération de VQE ou de QAOA nécessite que le CPU envoie de nouveaux paramètres au QPU, que le QPU exécute le circuit et renvoie les résultats de mesure, et que le CPU analyse ces résultats pour calculer le prochain jeu de paramètres. Si cette boucle de rétroaction prend des millisecondes ou des secondes, elle peut complètement anéantir tout avantage de vitesse potentiel que le calcul quantique aurait pu offrir, car l\'état quantique aura décohérence depuis longtemps.

Il est donc impératif de développer des métriques qui isolent et quantifient spécifiquement cette surcharge de communication. Au lieu de simplement mesurer le débit (comme CLOPS), nous avons besoin de métriques de **latence de boucle hybride**. Un tel benchmark pourrait consister en une tâche simple, comme l\'exécution d\'un circuit à un seul paramètre, et mesurer le temps \"mur à mur\" nécessaire pour effectuer 100 itérations d\'une boucle d\'optimisation simple. Ce \"temps par itération hybride\" deviendrait un indicateur clé de la capacité d\'un système à exécuter efficacement des algorithmes variationnels.

De plus, la performance de la pile logicielle, en particulier du compilateur, doit être évaluée. Des compilateurs plus intelligents peuvent réduire considérablement la profondeur du circuit et le nombre de portes à deux qubits en effectuant un routage de qubits plus efficace et en utilisant des techniques de synthèse de circuits avancées. Des suites de benchmarks spécifiquement conçues pour le logiciel, comme Benchpress, sont en cours de développement pour mesurer et comparer systématiquement les performances des différents SDK quantiques sur des tâches de construction, de manipulation et d\'optimisation de circuits à grande échelle. Ces efforts sont cruciaux, car une meilleure compilation peut apporter des gains de performance équivalents à des mois, voire des années, de progrès matériel. Des efforts de standardisation, comme ceux menés par l\'IEEE, visent à définir des architectures techniques et des API pour les environnements de calcul hybrides afin de garantir l\'interopérabilité et de permettre des comparaisons de performance équitables.

### 17.7 Niveau 3 : Métriques au Niveau Algorithmique

Le niveau ultime de l\'évaluation de la performance consiste à mesurer directement la capacité d\'un ordinateur quantique à résoudre des problèmes qui intéressent les utilisateurs finaux. Les benchmarks applicatifs abandonnent les circuits aléatoires au profit de circuits dérivés d\'algorithmes quantiques connus et pertinents. Ils visent à répondre à la question la plus importante : \"Dans quelle mesure cette machine est-elle performante pour la tâche qui m\'intéresse?\"

#### 17.7.1 Les suites de bancs d\'essai (benchmarks) applicatifs : QASMBench, SupermarQ

Plusieurs suites de benchmarks applicatifs ont vu le jour pour fournir une évaluation plus nuancée et plus pertinente que les métriques de niveau système.

- **QASMBench :** Il s\'agit d\'une suite de benchmarks de bas niveau, fournie sous forme de circuits en OpenQASM, le langage d\'assemblage quantique. Elle rassemble une large collection de noyaux et de routines quantiques couramment utilisés dans divers domaines, notamment la chimie quantique, la simulation, l\'algèbre linéaire, l\'optimisation et l\'apprentissage automatique. En étant de bas niveau, QASMBench est particulièrement utile pour évaluer l\'efficacité des compilateurs, des planificateurs et des simulateurs quantiques. En plus des métriques standard comme la largeur et la profondeur, QASMBench propose des métriques de circuit plus fines, telles que la densité de portes, la durée de vie de rétention (le temps maximal qu\'un qubit doit maintenir son état entre deux opérations) et la variance de l\'intrication, afin de mieux caractériser la susceptibilité d\'un circuit aux erreurs NISQ.
- **SupermarQ :** Contrairement à QASMBench, SupermarQ est une suite de benchmarks de haut niveau, agnostique du matériel, qui se concentre sur des applications de bout en bout. Elle comprend des benchmarks basés sur des algorithmes pertinents pour l\'ère NISQ, tels que le VQE, le QAOA et la simulation hamiltonienne. Une caractéristique distinctive de SupermarQ est son approche pour assurer la diversité et la représentativité. Chaque application de la suite est caractérisée par un \"vecteur de caractéristiques\" qui capture ses propriétés structurelles (par exemple, le degré d\'interaction des qubits, le parallélisme, la densité de portes). Cela permet d\'analyser la \"couverture\" de la suite de benchmarks et de s\'assurer qu\'elle teste un large éventail de types de circuits, ce qui rend plus difficile la sur-optimisation d\'un système pour la suite.

D\'autres suites, comme celles développées par le consortium QED-C, suivent une approche similaire, en fournissant des benchmarks qui balayent une gamme de tailles de problèmes pour caractériser la performance en termes de qualité et de temps d\'exécution dans des scénarios applicatifs reconnaissables.

**Analyse critique :**

- **Forces :** L\'avantage évident des benchmarks applicatifs est leur pertinence directe. Ils mesurent ce que les utilisateurs veulent réellement savoir : la performance sur des tâches qui ressemblent à leurs propres problèmes. Ils permettent une évaluation multidimensionnelle qui va au-delà d\'un seul chiffre, en examinant la qualité, la vitesse et les ressources consommées.
- **Faiblesses :** La pertinence a un coût : la généralité. La performance sur un benchmark VQE pour la chimie peut ne pas être un bon prédicteur de la performance sur un benchmark de recherche de Grover. Cela conduit à un tableau de bord de résultats plutôt qu\'à un seul chiffre facile à comparer. De plus, le choix des applications à inclure dans la suite est subjectif et peut introduire un biais. Si une suite se concentre uniquement sur des algorithmes qui se mappent bien sur une architecture particulière, elle favorisera injustement cette architecture.

**Pertinence pour la Q-AGI :** Ces suites sont le précurseur direct de ce qui sera nécessaire pour la Q-AGI. Une future suite \"Q-AGI-Bench\" devra être développée, contenant des tâches canoniques d\'apprentissage automatique quantique (par exemple, classification sur des ensembles de données de référence), d\'apprentissage par renforcement (par exemple, résoudre des environnements de contrôle standard) et de modélisation générative. C\'est à ce niveau que les véritables progrès vers une intelligence quantique seront mesurés. Le tableau suivant présente un aperçu de ces suites de benchmarks.

---

  Suite de Benchmarks   Philosophie                                                                      Applications Cibles                                            Métriques Principales                                                               Avantages et Inconvénients

  **QASMBench**         Niveau assembleur (OpenQASM), axé sur les noyaux d\'algorithmes.                 Chimie, optimisation, ML, cryptographie, etc.                  Métriques de circuit (densité de portes, durée de vie de rétention), fidélité.      **Avantages :** Proche du matériel, utile pour évaluer les compilateurs. **Inconvénients :** Moins représentatif de la performance de bout en bout.

  **SupermarQ**         Niveau application, agnostique du matériel, axé sur la diversité des circuits.   VQE, QAOA, simulation hamiltonienne.                           Score basé sur la distance de Hellinger, vecteurs de caractéristiques du circuit.   **Avantages :** Pertinence applicative, approche systématique de la diversité. **Inconvénients :** La sélection des applications peut introduire un biais.

  **QED-C Suite**       Niveau application, inspiré des benchmarks SPEC classiques.                      Simulation hamiltonienne, VQE, QFT, estimation d\'amplitude.   Qualité de la solution, temps d\'exécution total.                                   **Avantages :** Axé sur l\'expérience utilisateur, balayage des tailles de problèmes. **Inconvénients :** La performance peut être très dépendante de la pile logicielle spécifique.

---

**Table 17.3: Panorama des Suites de Benchmarks Applicatifs**

#### 17.7.2 Analyse de la pertinence des circuits choisis

La conception d\'une suite de benchmarks applicatifs est un art délicat. Le choix des circuits est d\'une importance capitale, car il détermine ce qui est réellement mesuré et peut influencer la direction du développement matériel et logiciel. Une critique fondamentale des benchmarks existants est qu\'ils peuvent encourager la \"sur-optimisation\" ou \"l\'enseignement à l\'épreuve\" (\"teaching to the test\"). Si une suite de benchmarks devient un standard de l\'industrie, les fabricants de matériel et les développeurs de logiciels seront fortement incités à optimiser leurs systèmes pour exceller sur cette suite spécifique de circuits. Cela pourrait conduire à des améliorations qui ne se généralisent pas à d\'autres applications, donnant une fausse impression de progrès général.

La stratégie de SupermarQ, qui consiste à caractériser les circuits par des vecteurs de caractéristiques et à viser une large couverture de cet \"espace de caractéristiques\", est une tentative de contrer ce problème. En incluant des circuits avec des structures de communication très différentes (par exemple, certains avec des interactions locales, d\'autres avec des interactions globales), des profondeurs variables et des densités de portes différentes, la suite devient plus difficile à \"tromper\". Un système doit être véritablement polyvalent pour bien performer sur l\'ensemble de la suite.

Une autre critique concerne la focalisation actuelle sur les algorithmes de l\'ère NISQ, principalement les algorithmes variationnels comme le VQE et le QAOA. Bien que ce soit pragmatique étant donné les limitations du matériel actuel, cela risque de créer un biais. Les architectures matérielles et les compilateurs pourraient être optimisés pour ces types de circuits peu profonds et bruités, potentiellement au détriment de la performance sur les algorithmes de l\'ère de la tolérance aux pannes, qui auront des structures très différentes (par exemple, des circuits très profonds dominés par des portes de Clifford pour la correction d\'erreurs). Une suite de benchmarks robuste et pérenne doit donc être adaptative et évolutive, intégrant de nouveaux algorithmes au fur et à mesure de leur découverte et de leur pertinence croissante.

#### 17.7.3 Métriques de performance : Qualité de la solution, temps-vers-la-solution, probabilité de succès

En fin de compte, pour un utilisateur d\'un algorithme quantique, trois questions priment sur toutes les autres : Ai-je obtenu la bonne réponse? Combien de temps cela a-t-il pris? Et quelle était la probabilité que cela fonctionne? Ces trois dimensions --- qualité, temps et probabilité --- forment l\'espace des compromis pour l\'avantage quantique pratique.

- **Qualité de la solution :** Cette métrique évalue la \"justesse\" du résultat fourni par l\'algorithme. Sa définition dépend fortement du type de problème. Pour les problèmes d\'optimisation (par exemple, Max-Cut avec QAOA), la qualité est souvent mesurée par le **ratio d\'approximation**, c\'est-à-dire le rapport entre la valeur de la solution trouvée par l\'algorithme quantique et la valeur de la solution optimale (connue ou estimée). Pour les problèmes de simulation quantique (par exemple, trouver l\'état fondamental d\'une molécule avec VQE), la qualité peut être la**précision de l\'énergie** calculée par rapport à la valeur exacte, ou la **fidélité** de l\'état quantique préparé par rapport à l\'état fondamental théorique. Des métriques plus sophistiquées comme le**V-score** ont été proposées pour les problèmes d\'état fondamental, combinant l\'estimation de l\'énergie et sa variance pour fournir une mesure de qualité plus absolue.
- **Temps-vers-la-solution (Time-to-Solution, TTS) :** Cette métrique mesure le temps total d\'horloge murale nécessaire pour atteindre une solution d\'une qualité prédéfinie avec une probabilité de succès élevée (par exemple, 99%). Le TTS est une métrique de performance de bout en bout. Il ne comprend pas seulement le temps d\'exécution sur le QPU, mais aussi tout le temps de traitement classique (compilation, optimisation des paramètres, post-traitement des résultats) et la latence de communication entre les composants classiques et quantiques. C\'est souvent la métrique la plus pertinente pour évaluer un avantage de vitesse pratique.
- **Probabilité de succès :** Les algorithmes quantiques sont intrinsèquement probabilistes. Une seule exécution d\'un circuit ne donne qu\'un seul échantillon de la distribution de probabilité de sortie. La probabilité de succès est la probabilité, lors d\'une seule exécution, de mesurer une chaîne de bits qui correspond à une solution de haute qualité. Pour les algorithmes comme celui de Shor, il s\'agit de la probabilité de mesurer un résultat qui permet de déduire la période. Pour les algorithmes de recherche comme celui de Grover, c\'est la probabilité de mesurer l\'élément recherché. Comme cette probabilité est souvent inférieure à 1, l\'algorithme doit être exécuté plusieurs fois pour amplifier la confiance dans le résultat. Le nombre de répétitions nécessaires est inversement proportionnel à la probabilité de succès, ce qui a un impact direct sur le temps-vers-la-solution.

Ces trois métriques sont en tension. Un algorithme peut trouver une solution de très haute qualité, mais nécessiter un temps infini pour converger. Un autre peut être très rapide mais avoir une probabilité de succès si faible que le nombre de répétitions nécessaires le rend inefficace. Un benchmark applicatif complet doit évaluer la performance dans cet espace tridimensionnel, par exemple en rapportant le temps-vers-la-solution pour atteindre un ratio d\'approximation de 0.95 avec une probabilité de 99%.

## Partie III : La Conception de Bancs d\'Essai Pertinents

L\'élaboration d\'un cadre de mesure hiérarchique est une première étape cruciale, mais elle ne garantit pas en soi que les benchmarks seront utiles ou équitables. La conception de bons benchmarks est un défi en soi, qui nécessite une réflexion approfondie sur les principes qui garantissent leur pertinence et leur robustesse. De plus, il est essentiel de reconnaître le rôle central que l\'informatique classique continue de jouer, non seulement comme outil de développement, mais aussi comme étalon de performance.

### 17.8 Principes Fondamentaux pour un Bon Benchmarking

En s\'inspirant des décennies d\'expérience de la communauté de l\'informatique classique haute performance, notamment des organisations comme SPEC (Standard Performance Evaluation Corporation), nous pouvons distiller plusieurs principes fondamentaux qui devraient guider la conception des benchmarks quantiques.

- **Pertinence (Relevance) :** Un benchmark doit mesurer des performances sur des tâches qui sont représentatives des charges de travail réelles des utilisateurs. Les benchmarks basés sur des circuits aléatoires peuvent être utiles pour sonder les capacités génériques d\'un système, mais ils manquent de pertinence applicative. Les benchmarks de haut niveau doivent être dérivés de problèmes concrets ayant une valeur scientifique ou commerciale, tels que la simulation de matériaux, l\'optimisation logistique ou l\'apprentissage automatique.
- **Scalabilité (Scalability) :** Le domaine de l\'informatique quantique évolue rapidement. Un benchmark doit être scalable, c\'est-à-dire qu\'il doit pouvoir être adapté à des machines de plus en plus grandes et puissantes. Cela signifie généralement que le problème sous-jacent au benchmark doit pouvoir être paramétré par la taille (par exemple, le nombre de qubits ou la complexité de l\'instance du problème) pour rester un défi pertinent pour les générations futures de matériel.
- **Portabilité et Équité (Portability and Fairness) :** Un benchmark doit être agnostique à la plateforme et défini à un niveau d\'abstraction suffisamment élevé pour pouvoir être exécuté sur différentes architectures matérielles (ions piégés, supraconducteurs, photonique, etc.) sans favoriser indûment l\'une d\'entre elles. Cela garantit une comparaison équitable (\"apples-to-apples\"). Pour assurer l\'équité, les règles d\'exécution doivent être clairement définies, en spécifiant par exemple le niveau d\'optimisation logicielle autorisé. L\'approche \"base\" et \"peak\" de SPEC, où les exécutions \"base\" utilisent des options de compilation standardisées et les exécutions \"peak\" permettent des optimisations spécifiques, pourrait être un modèle à suivre.
- **Reproductibilité et Vérifiabilité :** Les résultats d\'un benchmark doivent être reproductibles par des tiers. Cela nécessite que la spécification du benchmark, le code source et les ensembles de données soient publiquement disponibles. De plus, dans la mesure du possible, le résultat d\'un benchmark quantique devrait être vérifiable classiquement, au moins pour les petites tailles de problème, afin de valider l\'exactitude du calcul quantique.
- **Résistance à la sur-optimisation (\"Teaching to the Test\") :** C\'est l\'un des défis les plus subtils. Si un benchmark devient trop influent, les concepteurs peuvent être tentés d\'optimiser leurs systèmes spécifiquement pour ce benchmark, au détriment de la performance générale. Pour contrer cela, une
  *suite* de benchmarks, composée de plusieurs applications diverses, est préférable à un seul benchmark. L\'utilisation de techniques de randomisation dans la génération des instances de problèmes peut également rendre la sur-optimisation plus difficile.

Le respect de ces principes est un exercice d\'équilibre. Par exemple, une pertinence applicative très spécifique peut nuire à la portabilité. La scalabilité peut entrer en conflit avec la vérifiabilité classique. La conception d\'une suite de benchmarks mature et robuste est un processus itératif qui nécessitera la collaboration et le consensus de l\'ensemble de la communauté de recherche.

### 17.9 Le Rôle Essentiel et Continu de la Simulation Classique

Loin d\'être rendue obsolète par l\'avènement des processeurs quantiques, l\'informatique classique, et en particulier la simulation de systèmes quantiques sur des supercalculateurs, joue un rôle double et indispensable dans le développement de l\'informatique quantique. Elle est à la fois le principal outil de validation et l\'adversaire de référence.

#### 17.9.1 La simulation classique comme outil de vérification et de validation

À l\'ère NISQ, les ordinateurs quantiques sont, par définition, bruités et ne disposent pas de correction d\'erreurs. Par conséquent, leurs résultats sont intrinsèquement imparfaits. Comment savoir si le résultat d\'un calcul quantique est correct? La seule façon de le faire avec certitude est de comparer le résultat expérimental à la \"vérité terrain\" (ground truth) obtenue en simulant le même circuit quantique idéal sur un ordinateur classique.

Ce rôle de vérification est omniprésent :

- **Validation des benchmarks :** Des métriques comme le Volume Quantique ou la fidélité dans les benchmarks applicatifs reposent explicitement sur une comparaison avec une simulation classique pour évaluer le succès.
- **Débogage des algorithmes :** Lors du développement d\'un nouvel algorithme quantique, le faire tourner sur un simulateur est une première étape essentielle pour s\'assurer que la logique est correcte avant de le déployer sur un matériel bruité coûteux et difficile d\'accès.
- **Caractérisation du bruit :** En comparant les distributions de sortie expérimentales et simulées, les chercheurs peuvent inférer des caractéristiques sur le modèle de bruit du matériel.
- **Accès à l\'état complet :** Un avantage unique des simulateurs est leur capacité à donner accès au vecteur d\'état quantique complet à n\'importe quelle étape du calcul. C\'est une capacité de débogage et d\'analyse extrêmement puissante qu\'il est fondamentalement impossible d\'obtenir sur un vrai matériel quantique, où toute mesure projette et perturbe l\'état.

Bien sûr, la simulation exacte d\'un système de n qubits nécessite des ressources classiques qui croissent exponentiellement avec n. Cela limite la simulation d\'état-vecteur à environ 40-50 qubits, même sur les plus grands supercalculateurs. Cependant, même au-delà de cette limite, des techniques de simulation approximatives, comme les réseaux de tenseurs, peuvent souvent simuler efficacement des classes importantes de circuits quantiques et continuer à jouer un rôle de validation crucial.

#### 17.9.2 Le supercalculateur classique comme \"adversaire\" à battre pour prouver l\'avantage

Le second rôle de l\'informatique classique est celui d\'étalon de performance. Comme discuté précédemment (section 17.4.2), un avantage quantique n\'a de sens que s\'il surpasse le meilleur effort classique possible. Le supercalculateur classique n\'est donc pas seulement un outil de développement, mais l\'adversaire direct dans la course à la performance.

Cette dynamique contradictoire est une force motrice pour l\'innovation dans les deux domaines. Chaque fois qu\'une expérience quantique prétend avoir atteint un régime de calcul inaccessible aux machines classiques, elle lance un défi à la communauté HPC. En réponse, les chercheurs en algorithmes classiques développent de nouvelles techniques de simulation plus astucieuses qui repoussent les limites de ce qui est considéré comme \"classiquement intraitable\". Ce cycle a été observé à plusieurs reprises, notamment en réponse aux expériences de suprématie de Google et d\'autres.

Cela signifie que toute affirmation d\'avantage quantique doit être considérée comme provisoire et soumise à un examen minutieux constant. Un benchmark quantique rigoureux doit donc inclure une composante classique tout aussi rigoureuse : une évaluation de la performance du meilleur algorithme classique connu pour le même problème, exécuté sur une plateforme HPC de pointe. L\'objectif n\'est pas de battre un code classique \"de paille\", mais de démontrer une supériorité face à un adversaire optimisé et redoutable. L\'établissement d\'un avantage quantique durable est donc un effort profondément interdisciplinaire, qui se situe à l\'intersection de la physique quantique, de l\'informatique théorique et du calcul haute performance.

## Partie IV : Métriques et Bancs d\'Essai Spécifiques à l\'AGI Quantique

Alors que les parties précédentes ont jeté les bases d\'un benchmarking rigoureux pour les ordinateurs quantiques en général, la quête de l\'intelligence artificielle générale quantique (Q-AGI) introduit des défis d\'évaluation uniques et encore plus complexes. Une Q-AGI ne sera pas simplement un calculateur rapide ; elle devra apprendre, s\'adapter, généraliser et potentiellement faire preuve de capacités cognitives émergentes. L\'évaluation de telles capacités exige d\'aller bien au-delà des métriques de fidélité de circuit pour s\'inspirer des domaines de l\'apprentissage automatique, de la psychométrie et de l\'intelligence artificielle. Cette dernière partie explore les frontières du benchmarking, en esquissant les métriques et les environnements de test qui seront nécessaires pour mesurer de manière crédible les progrès vers une véritable intelligence quantique.

### 17.10 Évaluer la Performance des Modèles d\'Apprentissage Automatique Quantique

L\'apprentissage automatique quantique (QML) est l\'un des piliers les plus prometteurs de la Q-AGI. Cependant, l\'évaluation de la performance des modèles QML ne peut se contenter de la simple précision de classification, une leçon durement apprise par la communauté de l\'apprentissage automatique classique.

#### 17.10.1 Au-delà de la précision : Métriques de généralisation, de robustesse aux attaques adversariales, et d\'efficacité en données

La précision d\'un modèle sur un ensemble de test --- le pourcentage de prédictions correctes --- est une métrique nécessaire mais largement insuffisante. Un modèle véritablement intelligent doit posséder des qualités plus profondes.

- **Généralisation :** La capacité de généralisation est la capacité d\'un modèle à bien performer sur des données nouvelles et invisibles, après avoir été entraîné sur un ensemble de données limité. Une mauvaise généralisation, ou sur-apprentissage (overfitting), se produit lorsqu\'un modèle \"mémorise\" les données d\'entraînement au lieu d\'apprendre les motifs sous-jacents. Des recherches récentes ont montré que les modèles QML, comme leurs homologues classiques, peuvent avoir une capacité de mémorisation surprenante, étant capables d\'ajuster parfaitement des données aléatoires. Cela remet en question les approches traditionnelles pour comprendre la généralisation basée sur des mesures de complexité du modèle. La métrique clé ici est le **fossé de généralisation (generalization gap)** : la différence entre la performance sur l\'ensemble d\'entraînement et la performance sur l\'ensemble de test. Un petit fossé indique une bonne généralisation.
- **Robustesse :** Un modèle robuste est un modèle dont les prédictions ne changent pas radicalement en réponse à de petites perturbations non pertinentes des entrées. La robustesse aux **attaques adversariales**, où des perturbations infimes et souvent imperceptibles sont délibérément conçues pour tromper le modèle, est un test de résistance crucial. Les benchmarks de robustesse pour les modèles QML devraient inclure des ensembles de données de test contenant de tels exemples adversariaux et mesurer la dégradation de la performance. Des bornes théoriques, comme les bornes de Lipschitz, peuvent être utilisées pour quantifier la robustesse d\'un modèle et guider la conception de stratégies d\'entraînement qui l\'améliorent.
- **Efficacité en données (Data Efficiency) :** L\'un des avantages potentiels des modèles QML est leur capacité à apprendre à partir de très petites quantités de données, en exploitant le vaste espace de caractéristiques de Hilbert. Les benchmarks devraient donc évaluer la performance des modèles en fonction de la taille de l\'ensemble d\'entraînement, en traçant des courbes d\'apprentissage qui montrent comment la performance de généralisation s\'améliore à mesure que davantage de données sont disponibles. Un modèle qui atteint une haute performance avec très peu d\'échantillons démontrerait un avantage pratique significatif.

#### 17.10.2 Bancs d\'essai pour les noyaux quantiques et les cartographies de caractéristiques

Une classe importante de modèles QML est celle des méthodes à noyau quantique, comme les machines à vecteurs de support quantiques (QSVM). Ces méthodes fonctionnent en utilisant une **cartographie de caractéristiques quantiques** (quantum feature map) pour encoder les données classiques dans un état quantique, puis en utilisant le produit interne de ces états (la **fidélité**) comme une fonction noyau pour mesurer la similarité entre les points de données.

La question centrale pour le benchmarking de ces méthodes n\'est pas seulement de savoir si elles sont précises, mais si la cartographie quantique offre un réel avantage par rapport aux noyaux classiques. Un bon benchmark pour les noyaux quantiques devrait donc inclure :

1. **Comparaison des performances :** Évaluer la performance de classification (par exemple, score F1, aire sous la courbe ROC) du noyau quantique par rapport à une batterie de noyaux classiques de pointe (par exemple, RBF, polynomial) sur une collection diversifiée d\'ensembles de données du monde réel.
2. **Analyse géométrique :** Aller au-delà de la performance et analyser la géométrie de l\'espace de caractéristiques induit par la cartographie quantique. Des techniques comme l\'alignement de noyau peuvent être utilisées pour mesurer à quel point le noyau quantique est \"différent\" des noyaux classiques. L\'objectif est de déterminer si le processeur quantique \"perçoit\" les données d\'une manière fondamentalement nouvelle et utile, inaccessible aux méthodes classiques.
3. **Évaluation de l\'expressivité et de l\'entraînabilité :** Les cartographies de caractéristiques sont souvent des circuits paramétrés. Les benchmarks doivent évaluer la capacité de ces circuits à être entraînés efficacement (en évitant les plateaux stériles) et leur expressivité (leur capacité à générer une large gamme de fonctions noyau).

### 17.11 Évaluer les Agents d\'Apprentissage par Renforcement Quantique

L\'apprentissage par renforcement (RL) est un paradigme d\'apprentissage par l\'interaction qui est fondamental pour le développement d\'agents autonomes. L\'apprentissage par renforcement quantique (QRL) explore comment les principes quantiques pourraient améliorer les algorithmes de RL. Le benchmarking dans ce domaine est naissant et fait face à des défis considérables.

#### 17.11.1 Métriques d\'efficacité d\'exploration et de convergence vers la politique optimale

Le benchmarking en RL classique est déjà un domaine complexe sans consensus universel sur les meilleures métriques. Pour le QRL, où les affirmations d\'avantage ont souvent manqué de rigueur statistique, l\'établissement d\'une méthodologie solide est primordial. Les métriques clés pour évaluer les agents QRL incluent :

- **Efficacité d\'échantillonnage (Sample Efficiency) :** C\'est peut-être la métrique la plus importante. Elle mesure le nombre d\'interactions (échantillons) avec l\'environnement dont un agent a besoin pour atteindre un certain niveau de performance. Un avantage quantique pourrait se manifester par une réduction significative de la complexité d\'échantillonnage, permettant à l\'agent d\'apprendre beaucoup plus rapidement.
- **Vitesse de convergence :** Liée à l\'efficacité d\'échantillonnage, elle mesure le temps d\'horloge murale ou le nombre d\'itérations de mise à jour de la politique nécessaires pour converger vers une politique performante.
- **Performance finale de la politique :** La récompense cumulative moyenne obtenue par l\'agent une fois que son apprentissage a convergé. La question ici est de savoir si l\'agent quantique converge vers une politique *meilleure* (plus performante) que celle trouvée par son homologue classique, ou simplement vers la même politique mais plus rapidement.

Une méthodologie de benchmarking rigoureuse pour le QRL doit être statistique, en comparant les distributions de performance sur de nombreuses exécutions indépendantes pour tenir compte de la stochasticité de l\'apprentissage et de l\'environnement.

#### 17.11.2 Conception d\'environnements de test standardisés (ex: versions quantiques des benchmarks classiques comme OpenAI Gym)

Le progrès en RL classique a été énormément accéléré par la création de suites de benchmarks standardisées comme OpenAI Gym. Ces suites fournissent une collection d\'environnements de test diversifiés avec une interface unifiée, permettant aux chercheurs de comparer directement leurs algorithmes.

Une initiative similaire est désespérément nécessaire pour le QRL. Cela pourrait prendre deux formes :

1. **Agents quantiques dans des environnements classiques :** La première étape, déjà en cours, consiste à tester des agents QRL sur les environnements classiques bien établis de Gym, comme CartPole, Acrobot ou LunarLander. Cela permet une comparaison directe de la performance (par exemple, l\'efficacité d\'échantillonnage) entre les agents quantiques et classiques sur des tâches identiques.
2. **Environnements quantiques :** L\'étape la plus ambitieuse et la plus intéressante est la conception d\'une nouvelle suite d\'environnements de test qui sont eux-mêmes de nature quantique. Ces environnements pourraient impliquer des tâches telles que le contrôle d\'un système quantique bruité, la navigation dans un paysage énergétique complexe, ou la découverte de protocoles de correction d\'erreurs. Dans de tels environnements, un agent QRL pourrait avoir un avantage plus naturel, car il serait mieux adapté pour modéliser et interagir avec un monde fondamentalement quantique.

### 17.12 Évaluer les Systèmes Génératifs et Évolutionnaires Quantiques

Au-delà de l\'apprentissage discriminatif et du contrôle, une facette de l\'intelligence est la capacité à générer des artefacts nouveaux, créatifs et utiles. L\'évaluation des modèles génératifs quantiques pose des défis conceptuels profonds.

#### 17.12.1 Le défi de la mesure de la créativité, de la nouveauté et de la qualité des solutions générées

La \"créativité\" n\'est pas une quantité directement mesurable. Pour évaluer les modèles génératifs, qu\'ils soient classiques ou quantiques, nous devons nous appuyer sur des proxys quantifiables qui capturent différents aspects de la qualité générative.

- **Qualité et Fidélité :** Dans quelle mesure les échantillons générés sont-ils \"bons\" ou \"réalistes\"? Pour la génération d\'images, cela pourrait être mesuré par des scores comme le Fréchet Inception Distance (FID), qui compare la distribution statistique des caractéristiques des images générées à celle des images réelles. Pour la génération de structures moléculaires, la qualité pourrait être une fonction de la stabilité chimique et des propriétés souhaitées.
- **Diversité :** Le modèle génère-t-il une grande variété d\'échantillons différents, ou est-il bloqué dans quelques modes (mode collapse)? La diversité peut être mesurée par l\'entropie de la distribution des échantillons générés ou par la distance moyenne entre les échantillons d\'un même lot.
- **Nouveauté :** Le modèle génère-t-il des choses qu\'il n\'a pas vues dans l\'ensemble d\'entraînement? La nouveauté peut être quantifiée en mesurant la distance (par exemple, la divergence de Kullback-Leibler) entre la distribution des données générées et la distribution des données d\'entraînement. Une grande nouveauté suggère que le modèle a appris des principes sous-jacents plutôt que de simplement mémoriser.

Pour les algorithmes évolutionnaires quantiques, qui recherchent des solutions à des problèmes complexes, les métriques seraient similaires : la qualité de la meilleure solution trouvée, la diversité de la population de solutions explorées, et la nouveauté des solutions par rapport aux approches existantes. L\'évaluation de ces systèmes nécessitera probablement une combinaison de métriques automatisées et de jugement humain expert.

### 17.13 Vers des Bancs d\'Essai pour les Capacités Cognitives Émergentes

L\'objectif ultime de l\'AGI est l\'émergence de capacités cognitives de haut niveau, telles que le raisonnement abstrait, la planification à long terme et le transfert de connaissances. Mesurer ces capacités chez une intelligence artificielle, qu\'elle soit classique ou quantique, est la frontière de la recherche en évaluation de l\'IA.

#### 17.13.1 L\'adaptation des tests psychométriques pour évaluer le raisonnement abstrait, la planification et le transfert de connaissances des agents Q-AGI

L\'idée n\'est pas d\'administrer un test de QI humain à une Q-AGI, mais de s\'inspirer des principes de la psychométrie pour concevoir des tâches qui sondent des capacités cognitives spécifiques de manière objective et standardisée.

- **Raisonnement abstrait :** Inspiré par des tests comme les Matrices Progressives de Raven, on pourrait concevoir des benchmarks où l\'agent Q-AGI doit identifier le motif sous-jacent dans une séquence de données (par exemple, des états quantiques ou des graphes) et prédire l\'élément suivant. Le succès dans cette tâche indiquerait une capacité à généraliser à partir de règles abstraites plutôt que de caractéristiques de surface.
- **Planification :** Des environnements de test pourraient être conçus pour nécessiter une planification à long terme, où une séquence d\'actions mène à une récompense différée. La performance serait mesurée non seulement par la récompense finale, mais aussi par l\'efficacité de la politique trouvée (par exemple, la longueur du chemin vers la solution).
- **Transfert de connaissances (Transfer Learning) :** C\'est une caractéristique clé de l\'intelligence générale. Un benchmark de transfert de connaissances pourrait consister à entraîner un agent sur une série de tâches, puis à évaluer sa performance sur une nouvelle tâche, jamais vue auparavant, avec très peu ou pas d\'entraînement supplémentaire (\"zero-shot\" ou \"few-shot learning\"). La rapidité et l\'efficacité avec lesquelles l\'agent s\'adapte à la nouvelle tâche mesureraient sa capacité à transférer et à réutiliser les connaissances acquises.

La conception de tels benchmarks cognitifs pour la Q-AGI est un programme de recherche à long terme qui nécessitera une collaboration étroite entre physiciens quantiques, informaticiens de l\'IA et psychologues cognitifs. C\'est cependant une direction essentielle si nous voulons un jour être capables de répondre de manière rigoureuse à la question : \"Cette machine est-elle vraiment intelligente?\"

## 17.14 Conclusion : Mesurer pour Progresser

Au terme de cette analyse exhaustive des métriques et des bancs d\'essai pour l\'intelligence artificielle générale quantique, une conclusion s\'impose avec force : le chemin vers une Q-AGI crédible et démontrable est pavé de mesures rigoureuses. Dans un domaine où le potentiel est immense mais la réalité technique est complexe et semée d\'embûches, la capacité à mesurer objectivement les progrès n\'est pas un luxe, mais une nécessité absolue. Sans un compas fiable, la navigation dans le vaste et bruité paysage de l\'informatique quantique à échelle intermédiaire risque de s\'égarer dans les mirages de l\'hyperbole.

### 17.14.1 Synthèse : L\'établissement d\'un consensus autour d\'une suite de benchmarks riches et multi-niveaux est une étape indispensable pour la maturation du domaine

Ce chapitre a argumenté qu\'aucune métrique unique ne peut capturer la performance d\'un système aussi complexe qu\'un ordinateur quantique destiné à l\'AGI. En réponse, nous avons proposé un cadre hiérarchique et holistique, une suite de benchmarks multi-niveaux qui évalue la performance à chaque couche de la pile technologique.

- Au **niveau physique**, nous avons souligné l\'importance de caractériser les composants de base avec des métriques comme les temps de cohérence et la fidélité des portes, en utilisant des outils robustes comme le Randomized Benchmarking pour une évaluation scalable et la tomographie pour un diagnostic en profondeur.
- Au **niveau système**, nous avons analysé les forces et les faiblesses des métriques intégrées comme le Volume Quantique et le CLOPS, qui capturent les effets globaux de la connectivité et de la diaphonie, tout en soulignant le besoin critique de nouvelles métriques pour quantifier la latence de la communication classique-quantique, un goulot d\'étranglement majeur pour les algorithmes hybrides.
- Au **niveau algorithmique**, nous avons plaidé pour l\'adoption de suites de benchmarks applicatifs, comme QASMBench et SupermarQ, qui mesurent la performance sur des tâches pertinentes pour l\'utilisateur, en se concentrant sur le triptyque essentiel de la qualité de la solution, du temps-vers-la-solution et de la probabilité de succès.
- Enfin, en nous tournant vers les défis spécifiques de la **Q-AGI**, nous avons esquissé une feuille de route pour développer des benchmarks qui évaluent non seulement la précision, mais aussi la généralisation, la robustesse, l\'efficacité d\'échantillonnage et, à terme, les capacités cognitives émergentes.

L\'adoption d\'un tel cadre par la communauté internationale n\'est pas une simple question de standardisation technique. C\'est une étape indispensable à la maturation du domaine, qui permettra de passer d\'une phase d\'exploration qualitative à une phase d\'ingénierie quantitative et de progrès systématique.

### 17.14.2 La nécessité d\'une culture de la transparence, de la reproductibilité et de l\'honnêteté intellectuelle dans la communication des résultats

Le cadre technique que nous avons proposé ne pourra porter ses fruits que s\'il est soutenu par un changement culturel. La course à l\'avantage quantique ne doit pas devenir une \"course aux benchmarks\" où les résultats sont présentés de manière sélective pour maximiser l\'impact médiatique au détriment de la rigueur scientifique.

Une culture de la transparence est nécessaire. Cela signifie que les chercheurs et les entreprises doivent publier non seulement leurs meilleurs résultats, mais aussi les détails complets de leurs méthodes, y compris les techniques d\'optimisation et d\'atténuation d\'erreurs utilisées, qui peuvent avoir un impact considérable sur la performance rapportée. Les codes sources des benchmarks et les données brutes devraient être rendus publics pour permettre la reproductibilité et la vérification par des tiers.

Une culture de l\'honnêteté intellectuelle est également primordiale. Cela implique de reconnaître les limites de chaque métrique, de comparer les performances quantiques aux meilleurs et plus récents algorithmes classiques, et de résister à la tentation de déclarer prématurément un \"avantage\" avant qu\'il ne soit solidement établi et validé par la communauté au sens large. L\'établissement de consortiums et d\'organismes de standardisation, à l\'image de ce qui existe dans le monde du HPC, sera un pas important dans cette direction.

### 17.14.3 Transition vers le chapitre 18 : Forts d\'un cadre de mesure rigoureux, nous pouvons maintenant esquisser les perspectives du domaine avec plus de clarté et de crédibilité

En conclusion, mesurer, c\'est comprendre. En nous dotant d\'outils de mesure sophistiqués, nuancés et honnêtes, nous nous donnons les moyens de comprendre véritablement les forces et les faiblesses de nos technologies quantiques actuelles. Cette compréhension est le fondement sur lequel nous pouvons construire des feuilles de route réalistes, identifier les goulots d\'étranglement les plus critiques à résoudre, et allouer les ressources de recherche et développement de la manière la plus efficace.

Forts de ce cadre de mesure rigoureux, nous sommes désormais en bien meilleure position pour nous tourner vers l\'avenir. Le chapitre suivant s\'appuiera sur cette fondation métrologique pour esquisser les perspectives futures du domaine de la Q-AGI, non pas comme une série de spéculations optimistes, mais comme une projection crédible et fondée sur des données, traçant un chemin plausible de l\'ère NISQ bruitée vers l\'aube potentielle d\'une nouvelle forme d\'intelligence.

