Le troisième volume examine l\'écosystème logiciel qui anime le matériel et fournit les outils nécessaires au développement d\'applications. Il couvre les **Logiciels Système**, notamment les Systèmes d\'Exploitation (OS), qui gèrent les ressources matérielles, la concurrence et la mémoire. Il explore également la théorie et la pratique de la **Compilation** et la conception des langages de programmation. Une part importante est dédiée aux **Algorithmes et Structures de Données**, le répertoire essentiel pour résoudre efficacement les problèmes computationnels. Enfin, le volume aborde le **Génie Logiciel**, discipline cruciale pour gérer la complexité du développement de systèmes à grande échelle, en mettant l\'accent sur les méthodologies (Agile, DevOps), l\'architecture logicielle et l\'assurance qualité.

# Chapitre 16 : Systèmes d\'Exploitation (OS) - Concepts Fondamentaux

## Introduction : Le Double Visage du Système d\'Exploitation

Un système d\'exploitation (OS) est l\'ensemble de programmes le plus fondamental et le plus complexe d\'un ordinateur. Il constitue l\'intermédiaire indispensable entre le matériel physique et les applications logicielles, remplissant un double rôle qui, à première vue, peut sembler paradoxal : celui de simplificateur et celui de contrôleur. Comprendre cette dualité est essentiel pour saisir la nature et la fonction de tout système d\'exploitation moderne.

### L\'OS comme Machine Étendue : Le Principe d\'Abstraction

La première perspective conçoit le système d\'exploitation comme une **machine étendue** ou une **machine virtuelle**. Le matériel informatique brut est une collection complexe de circuits, de registres et de périphériques, chacun avec son propre langage de bas niveau et ses propres particularités. Interagir directement avec ce matériel serait une tâche d\'une complexité prohibitive pour la plupart des programmeurs. Le système d\'exploitation intervient pour masquer cette complexité, offrant à l\'utilisateur et aux applications une vision simple et unifiée de la machine.

Ce rôle peut être décrit par l\'expression anglophone de *\"beautification principle\"* : le système d\'exploitation embellit la réalité matérielle. Par exemple, plutôt que de manipuler les têtes de lecture/écriture d\'un disque dur, un programmeur interagit avec une abstraction simple : le fichier. Il utilise des commandes génériques comme

read et write, et c\'est l\'OS qui se charge de traduire ces requêtes en instructions spécifiques pour le contrôleur de disque. De même, l\'OS gère les interruptions, les horloges et les mécanismes de bas niveau de la mémoire, soulageant les programmeurs des disparités entre les différentes architectures matérielles. En somme, il \"donne vie à la machine\"  en la transformant en un environnement de programmation cohérent et accessible.

### L\'OS comme Gestionnaire de Ressources : Arbitrage et Contrôle

La seconde perspective, complémentaire à la première, est celle de l\'OS en tant que **gestionnaire de ressources**. Un ordinateur dispose d\'un ensemble fini de ressources : le temps processeur (CPU), la mémoire vive (RAM), l\'espace de stockage et les divers périphériques d\'entrée/sortie. Dans un environnement où de multiples programmes et utilisateurs coexistent et se disputent ces ressources, un accès non régulé mènerait inévitablement à des conflits et au chaos.

Le rôle de l\'OS est donc d\'arbitrer l\'accès à ces ressources de manière ordonnée et efficace. Il alloue le CPU aux différents programmes, gère l\'espace mémoire pour éviter les interférences, contrôle l\'accès aux fichiers et aux périphériques, et résout les conflits lorsque plusieurs entités demandent la même ressource simultanément. L\'objectif est de maximiser l\'utilisation des ressources, d\'assurer une performance optimale du système et de garantir un environnement stable et fiable pour l\'exécution des programmes. Cette fonction de gestion implique également la protection des ressources contre les accès non autorisés, un pilier de la sécurité informatique.

Cette dualité entre abstraction (cacher la complexité) et contrôle (gérer les détails) est au cœur de la conception des systèmes d\'exploitation. Un appel système, par exemple, illustre parfaitement cette tension : il offre une fonction simple à l\'utilisateur (comme open() pour ouvrir un fichier) mais son exécution déclenche un mécanisme de contrôle strict qui implique une vérification des permissions et une gestion fine des ressources par le noyau. Chaque décision de conception d\'un OS est un arbitrage entre ces deux pôles, cherchant un équilibre entre performance, sécurité et simplicité d\'utilisation.

### La Dichotomie Fondamentale : Espace Noyau et Espace Utilisateur

Pour remplir ses fonctions de contrôle et de protection, le système d\'exploitation s\'appuie sur une séparation matérielle fondamentale : la distinction entre le **mode noyau** (ou mode superviseur) et le **mode utilisateur**. Les processeurs modernes implémentent plusieurs niveaux de privilèges, souvent appelés anneaux de protection (

*rings*).

Le **mode noyau** est le niveau de privilège le plus élevé (anneau 0 sur l\'architecture x86). Le code qui s\'exécute dans ce mode, c\'est-à-dire le noyau de l\'OS, a un accès illimité à toutes les instructions du processeur et à l\'ensemble des ressources matérielles et de la mémoire. C\'est dans cet espace que résident les fonctions les plus critiques du système.

À l\'inverse, le **mode utilisateur** est un mode d\'exécution restreint (anneau 3 sur x86). Les applications des utilisateurs s\'exécutent dans ce mode, où les instructions potentiellement dangereuses (comme celles qui modifient la configuration du système ou accèdent directement au matériel) sont interdites. Toute tentative d\'exécuter une telle instruction ou d\'accéder à une zone mémoire protégée (appartenant au noyau ou à un autre programme) depuis le mode utilisateur déclenche une erreur matérielle, une \"trappe\", qui redonne immédiatement le contrôle au noyau.

Cette séparation est la pierre angulaire de la stabilité et de la sécurité des systèmes modernes. Elle crée une barrière infranchissable qui empêche une application défaillante ou malveillante de corrompre le noyau ou d\'interférer avec les autres applications, garantissant ainsi l\'intégrité du système dans son ensemble.

## Au Cœur du Système -- Architectures du Noyau

Le noyau est le composant central du système d\'exploitation, celui qui s\'exécute en mode privilégié et assure les fonctions les plus fondamentales. La manière dont ce noyau est structuré, son architecture, a des implications profondes sur la performance, la stabilité et la flexibilité du système tout entier. Historiquement, trois grandes philosophies architecturales ont émergé : monolithique, micronoyau et hybride.

### Le Noyau Monolithique : Performance et Intégration

Dans une architecture monolithique, l\'ensemble des services principaux du système d\'exploitation --- gestion des processus, gestion de la mémoire, systèmes de fichiers, pilotes de périphériques, pile réseau --- est regroupé en un seul grand programme exécuté dans un unique espace d\'adressage : l\'espace noyau. La communication entre ces différents composants est aussi simple et rapide que possible : il s\'agit d\'appels de fonctions directs au sein du même programme.

Cette intégration étroite confère aux noyaux monolithiques leur principal avantage : la **performance**. L\'absence de frontières entre les sous-systèmes élimine toute surcharge de communication, ce qui en fait l\'architecture la plus performante en termes de vitesse d\'exécution brute. Cependant, cette structure présente un inconvénient majeur en matière de

**robustesse**. Puisque tous les composants partagent le même espace mémoire sans protection mutuelle, une erreur dans un seul module, comme un pilote de périphérique défectueux, peut corrompre des données critiques et provoquer l\'arrêt complet du système (un \"kernel panic\").

#### Étude de cas : L\'architecture modulaire du noyau Linux

Le noyau Linux est l\'exemple le plus emblématique d\'un noyau monolithique réussi. Cependant, il a surmonté la rigidité des premières conceptions monolithiques en adoptant une **architecture modulaire**. Linux permet de charger et de décharger dynamiquement des modules de noyau à l\'exécution. Ces modules peuvent contenir des pilotes de périphériques, des protocoles réseau ou des systèmes de fichiers.

Cette modularité offre une flexibilité considérable : le noyau peut être configuré pour ne contenir que les fonctionnalités strictement nécessaires à un système donné, tout en permettant d\'ajouter de nouvelles fonctionnalités sans nécessiter une recompilation complète ou un redémarrage. Néanmoins, il est crucial de noter que bien que modulaire dans sa conception, Linux reste monolithique dans son exécution : une fois chargés, les modules s\'exécutent en mode noyau avec les mêmes privilèges que le reste du système, conservant ainsi les avantages de performance de l\'approche monolithique, mais aussi sa vulnérabilité aux modules défaillants.

### L\'Approche Micronoyau : Modularité et Sécurité

En réaction aux inconvénients des noyaux monolithiques, l\'approche micronoyau propose une philosophie radicalement différente. L\'objectif est de minimiser autant que possible la quantité de code s\'exécutant en mode noyau, le \"micro-noyau\" lui-même, en le limitant aux fonctions les plus essentielles. Typiquement, un micronoyau ne gère que la communication inter-processus (IPC), la gestion de base de la mémoire virtuelle et l\'ordonnancement des threads.

Tous les autres services traditionnels de l\'OS (systèmes de fichiers, pilotes de périphériques, pile réseau) sont implémentés comme des processus serveurs distincts, s\'exécutant en mode utilisateur, comme de simples applications. La communication entre ces serveurs, ou entre une application et un serveur, se fait exclusivement par l\'échange de messages, un service fourni et arbitré par le micronoyau.

Cette architecture offre des avantages significatifs en termes de **robustesse** et de **sécurité**. Si un pilote de périphérique plante, seul son processus serveur est affecté ; le reste du système, y compris le noyau et les autres services, continue de fonctionner. La maintenance est également simplifiée : un service peut être mis à jour, redémarré ou remplacé sans avoir à redémarrer l\'ensemble de la machine.

Le principal inconvénient de cette approche est la **performance**. Chaque interaction entre une application et un service, ou entre deux services, nécessite au moins deux transitions de mode (utilisateur → noyau → utilisateur) et une opération de passage de message via le noyau. Cette surcharge de communication (IPC) est intrinsèquement plus lente que de simples appels de fonctions au sein d\'un noyau monolithique, ce qui peut dégrader les performances globales du système.

#### Étude de cas : Le modèle de services du micronoyau Mach

Développé à l\'Université Carnegie Mellon à partir de 1985, Mach est l\'un des pionniers et des plus influents micronoyaux. Conçu pour la recherche sur les systèmes distribués et le calcul parallèle, il a introduit des abstractions puissantes qui ont influencé de nombreux systèmes ultérieurs. Ses concepts fondamentaux sont :

- **Tâche** : Un conteneur de ressources, principalement un espace d\'adressage et un ensemble de droits de communication.

- **Thread** : L\'unité d\'exécution au sein d\'une tâche.

- **Port** : Un canal de communication protégé pour l\'échange de messages (IPC). Toutes les ressources et tous les services sont représentés par des ports.

- **Objet Mémoire** : Une abstraction permettant à des serveurs en mode utilisateur de gérer le stockage persistant (backing store) pour la mémoire virtuelle.

### Les Noyaux Hybrides : La Voie du Compromis

Face aux inconvénients respectifs des deux approches pures, la plupart des systèmes d\'exploitation commerciaux modernes ont adopté une architecture hybride. Un noyau hybride est structuré conceptuellement comme un micronoyau, mais, pour des raisons de performance, il réintègre certains services critiques directement dans l\'espace noyau.

Par exemple, les sous-systèmes graphiques et certains pilotes de périphériques, qui nécessitent une très faible latence et une bande passante élevée, sont souvent déplacés de l\'espace utilisateur vers l\'espace noyau. Cela permet à ces composants de communiquer plus rapidement entre eux et avec le matériel, en évitant la surcharge des IPC, tout en essayant de conserver une structure modulaire pour les autres services. Les systèmes d\'exploitation comme Microsoft Windows (depuis NT) et macOS d\'Apple (avec son noyau XNU, qui combine des éléments du micronoyau Mach et du noyau monolithique BSD) sont des exemples typiques d\'architectures hybrides.

En réalité, la distinction autrefois nette entre les architectures s\'est estompée. Les noyaux monolithiques sont devenus modulaires pour gagner en flexibilité, tandis que les systèmes basés sur des micronoyaux sont devenus hybrides pour gagner en performance. Cette convergence illustre un pragmatisme d\'ingénierie où les avantages théoriques d\'une approche sont tempérés par les contraintes pratiques de performance et de complexité, menant à des solutions qui empruntent le meilleur des deux mondes.

  ---------------------------------- ----------------------------------------- ------------------------------------ ---------------------------------------- ----------------------------------------
  Caractéristique                    Noyau Monolithique                        Monolithique Modulaire (ex: Linux)   Micronoyau (ex: Mach, Minix)             Noyau Hybride (ex: Windows, macOS)

  **Performance**                    Très élevée                               Élevée                               Plus faible (surcharge IPC)              Élevée (compromis)

  **Robustesse/Stabilité**           Faible (un bug peut tout arrêter)         Faible                               Très élevée (services isolés)            Élevée

  **Sécurité**                       Plus faible (grande surface d\'attaque)   Plus faible                          Élevée (principe du moindre privilège)   Élevée

  **Communication Inter-services**   Très rapide (appel de fonction)           Très rapide (appel de fonction)      Lente (passage de messages)              Mixte (rapide en noyau, lente via IPC)

  **Extensibilité/Modularité**       Faible                                    Élevée (modules dynamiques)          Très élevée (services utilisateur)       Élevée

  **Taille du code en mode noyau**   Très grande                               Grande (mais adaptable)              Très petite                              Moyenne
  ---------------------------------- ----------------------------------------- ------------------------------------ ---------------------------------------- ----------------------------------------

## Le Pont entre les Mondes -- Le Mécanisme des Appels Système

La dichotomie entre le mode utilisateur et le mode noyau est essentielle à la protection du système, mais elle pose une question fondamentale : comment une application, confinée dans le mode utilisateur, peut-elle légitimement demander au noyau d\'effectuer une opération privilégiée pour elle, comme lire un fichier ou allouer de la mémoire? La réponse réside dans un mécanisme de transition contrôlé et hautement sécurisé : l\'**appel système** (*system call*).

### La Protection par les Modes d\'Exécution

Comme nous l\'avons vu, les processeurs modernes imposent une hiérarchie de privilèges, souvent matérialisée par des \"anneaux de protection\". Le noyau s\'exécute dans l\'anneau 0, le plus privilégié, tandis que les applications sont reléguées à l\'anneau 3, le moins privilégié. Cette barrière, imposée par le matériel, empêche une application d\'accéder directement à la mémoire du noyau ou d\'exécuter des instructions qui pourraient compromettre la stabilité du système. L\'appel système est le pont officiel et gardé qui permet de traverser cette frontière en toute sécurité.

### La Transition Contrôlée : Interruption Logicielle et Trappe (Trap)

Une application ne peut pas simplement exécuter un saut vers une adresse mémoire située dans l\'espace du noyau. Une telle tentative serait immédiatement bloquée par le processeur, générant une erreur de protection. Pour demander un service, le programme doit utiliser une instruction machine spéciale qui provoque une **interruption logicielle**, également appelée une **trappe** (*trap*).

Cette instruction est le seul moyen légitime pour un programme en mode utilisateur de transférer volontairement le contrôle au noyau. Son exécution déclenche une séquence d\'événements gérée par le matériel  :

1.  Le processeur sauvegarde l\'état d\'exécution actuel du programme (au minimum, le compteur de programme, qui contient l\'adresse de l\'instruction suivante).

2.  Le mode d\'exécution du processeur bascule de \"utilisateur\" à \"noyau\".

3.  Le processeur saute à une adresse fixe, prédéterminée par le noyau au démarrage. Cette adresse est lue depuis une structure de données spéciale appelée la table des vecteurs d\'interruption (*Interrupt Vector Table*).

4.  À cette adresse se trouve le code du gestionnaire d\'appels système du noyau, qui prend alors le contrôle.

### Séquence d\'un Appel Système : De la Bibliothèque C au Traitement par le Noyau

Pour le développeur d\'applications, ce mécanisme complexe est généralement invisible, masqué par les bibliothèques système comme la libc sur les systèmes de type Unix. La séquence complète d\'un appel système, par exemple

write(), est la suivante :

1.  **Appel de la fonction wrapper :** L\'application appelle la fonction write() de la libc. Ce n\'est pas encore le véritable appel système, mais une fonction \"enveloppe\" (*wrapper*) qui s\'exécute en mode utilisateur.

2.  **Préparation des paramètres :** La fonction wrapper prépare la transition. Elle place un numéro unique identifiant l\'appel système write dans un registre spécifique du processeur (par exemple, le registre eax sur l\'architecture x86). Elle place ensuite les arguments de la fonction (le descripteur de fichier, le pointeur vers les données, et la taille) dans d\'autres registres prédéfinis, conformément à une convention d\'appel binaire (ABI).

3.  **Exécution de la trappe :** La fonction wrapper exécute l\'instruction de trappe (par exemple, int 0x80 sur les anciens systèmes Linux x86, ou l\'instruction plus moderne syscall).

4.  **Transition vers le mode noyau :** Le matériel prend le relais, bascule en mode noyau et transfère le contrôle au gestionnaire d\'appels système du noyau.

5.  **Exécution dans le noyau :** Le gestionnaire lit le numéro d\'appel système dans le registre eax. Il utilise ce numéro comme un index dans une table interne, la sys_call_table, qui contient les adresses de toutes les fonctions du noyau implémentant les appels système. Il exécute alors la fonction\
    sys_write correspondante.

6.  **Retour au mode utilisateur :** Une fois la fonction sys_write terminée, elle retourne un résultat (le nombre d\'octets écrits ou un code d\'erreur). Le gestionnaire d\'appels système place cette valeur de retour dans un registre, puis exécute une instruction spéciale de retour d\'interruption. Cette instruction fait basculer le processeur en mode utilisateur et restaure le contexte du programme applicatif, qui reprend son exécution juste après l\'instruction de trappe.

7.  **Retour de la fonction wrapper :** La fonction wrapper de la libc récupère la valeur de retour depuis le registre et la retourne à l\'application.

Ce mécanisme complexe garantit que le code utilisateur ne peut jamais exécuter directement du code en mode noyau. Il ne peut que demander des services spécifiques et prédéfinis via un système de numéros. Cette indirection est une mesure de sécurité cruciale : l\'application ne peut pas spécifier une adresse à laquelle sauter, mais doit demander un service par son identifiant, que le noyau est libre de valider et d\'honorer ou de rejeter. L\'appel système fonctionne donc comme un contrat formel et inviolable entre les deux mondes, où le noyau reste le seul maître des opérations privilégiées.

### L\'API du Système : Une Interface Standardisée sur le Chaos Matériel

L\'ensemble de tous les appels système disponibles forme l\'**Interface de Programmation d\'Application (API)** du système d\'exploitation. Des standards comme POSIX définissent une API commune pour les systèmes de type Unix, spécifiant des appels système comme

open, read, fork, execve, etc.. Cette API fournit une interface stable et portable qui permet aux développeurs d\'écrire des programmes qui fonctionneront sur différents systèmes conformes, indépendamment des spécificités de leur matériel sous-jacent.

## Le Processus -- L\'Unité d\'Abstraction Fondamentale

Le concept de **processus** est l\'une des abstractions les plus fondamentales offertes par un système d\'exploitation. Il s\'agit de l\'instance d\'un programme en cours d\'exécution. Alors qu\'un programme est une entité passive, une collection d\'instructions stockée sur un disque, un processus est une entité dynamique, vivante, avec un état qui évolue dans le temps.

### Anatomie d\'un Processus : Espace d\'Adressage

Chaque processus s\'exécute dans son propre **espace d\'adressage virtuel**. Cet espace, géré par l\'OS en collaboration avec l\'unité de gestion de la mémoire (MMU) du processeur, donne au processus l\'illusion de disposer de toute la mémoire de la machine pour lui seul, de manière contiguë. Cette isolation empêche un processus de lire ou de modifier la mémoire d\'un autre processus ou du noyau, ce qui est un mécanisme de protection essentiel. L\'espace d\'adressage d\'un processus est typiquement structuré en plusieurs segments  :

- **Segment de texte (Code)** : Contient les instructions machine du programme. Cette zone est généralement en lecture seule pour empêcher le processus de modifier son propre code.

- **Segment de données** : Stocke les variables globales et statiques initialisées et non initialisées.

- **Tas (Heap)** : Une zone de mémoire qui peut croître et décroître dynamiquement pendant l\'exécution du processus. C\'est ici que la mémoire est allouée lors d\'appels à des fonctions comme malloc() en C.

- **Pile (Stack)** : Une zone de mémoire qui croît et décroît de manière LIFO (Last-In, First-Out). Elle est utilisée pour stocker les variables locales des fonctions, les paramètres passés aux fonctions et les adresses de retour lors des appels de fonction.

### Le Bloc de Contrôle du Processus (PCB) : La Carte d\'Identité

Pour gérer les multiples processus qui s\'exécutent simultanément, le noyau doit conserver des informations détaillées sur chacun d\'eux. Ces informations sont stockées dans une structure de données centrale appelée le **Bloc de Contrôle du Processus** (PCB, *Process Control Block*), également connu sous le nom de descripteur de processus. Le PCB est la manifestation concrète de l\'abstraction du processus ; sans lui, un programme en cours d\'exécution n\'aurait ni identité ni état pour le système d\'exploitation. Il est créé à la naissance du processus et réside dans une zone mémoire protégée, accessible uniquement au noyau.

Un PCB contient toutes les informations nécessaires au système pour suspendre et reprendre l\'exécution d\'un processus. Sa structure exacte varie d\'un OS à l\'autre, mais il inclut généralement les champs suivants  :

- **Identification du processus** : Un identifiant unique (PID), l\'identifiant du processus parent (PPID), l\'identifiant de l\'utilisateur (UID), etc..

- **État du processeur (contexte matériel)** : Le contenu des registres du CPU, notamment le compteur de programme (qui indique la prochaine instruction à exécuter) et le pointeur de pile. Ces informations sont cruciales pour reprendre l\'exécution du processus exactement là où elle a été interrompue.

- **État du processus** : L\'état actuel du processus (par exemple, prêt, en exécution, bloqué).

- **Informations d\'ordonnancement** : La priorité du processus, des pointeurs vers les files d\'attente d\'ordonnancement, et d\'autres paramètres utilisés par l\'ordonnanceur.

- **Informations de gestion de la mémoire** : Des pointeurs vers les tables de pages ou de segments qui décrivent l\'espace d\'adressage virtuel du processus.

- **Informations comptables** : Le temps CPU consommé, les limites de ressources, etc..

- **Informations sur les entrées/sorties** : Une liste des fichiers ouverts par le processus, les périphériques qui lui sont alloués, etc..

#### Étude de cas : task_struct sous Linux

Sous Linux, le PCB est implémenté par la structure C task_struct. C\'est une structure de données massive qui contient des centaines de champs, reflétant la complexité de la gestion d\'un processus moderne. Elle inclut non seulement les informations listées ci-dessus, mais aussi des pointeurs vers d\'autres structures de données clés, telles que

mm_struct (qui décrit l\'espace d\'adressage mémoire) et files_struct (qui contient la table des descripteurs de fichiers ouverts).

### Le Cycle de Vie d\'un Processus : Le Diagramme à Cinq États

Au cours de son exécution, un processus traverse une série d\'états. Un modèle classique, bien que simplifié, décrit ce cycle de vie à l\'aide de cinq états principaux  :

1.  **Nouveau (New)** : Le processus est en cours de création. L\'OS a alloué un PCB mais n\'a pas encore admis le processus dans le pool des processus exécutables.

2.  **Prêt (Ready)** : Le processus dispose de toutes les ressources nécessaires pour s\'exécuter, à l\'exception du CPU. Il est chargé en mémoire et attend dans une file d\'attente que l\'ordonnanceur le sélectionne.

3.  **Élu (Running)** : Le processus est actuellement en cours d\'exécution sur un cœur de processeur ; ses instructions sont exécutées.

4.  **Bloqué (Blocked / Waiting)** : Le processus ne peut pas continuer son exécution car il attend qu\'un événement externe se produise, comme la fin d\'une opération d\'E/S, la disponibilité d\'une ressource, ou un signal d\'un autre processus.

5.  **Terminé (Terminated)** : Le processus a achevé son exécution. L\'OS est en train de libérer les ressources qui lui étaient allouées.

Les transitions entre ces états sont des événements clés dans la vie d\'un processus. Par exemple, l\'ordonnanceur fait passer un processus de l\'état *Prêt* à *Élu*. Un processus passe de *Élu* à *Bloqué* lorsqu\'il initie une opération d\'E/S. Lorsqu\'une opération d\'E/S se termine, une interruption matérielle fait passer le processus de *Bloqué* à *Prêt*.

### La Commutation de Contexte : Mécanisme et Coûts Associés

Dans un système multitâche préemptif, le passage du CPU d\'un processus à un autre est une opération fondamentale appelée **commutation de contexte** (*context switch*). Ce mécanisme est ce qui donne l\'illusion que plusieurs programmes s\'exécutent simultanément sur un seul processeur.

Le mécanisme est le suivant  :

1.  Le système d\'exploitation décide d\'arrêter le processus P1 en cours d\'exécution et d\'en lancer un autre, P2.

2.  **Sauvegarde du contexte de P1** : L\'OS sauvegarde l\'état complet du processeur (tous les registres, le compteur de programme, etc.) dans le PCB du processus P1.

3.  **Mise à jour du PCB de P1** : L\'état de P1 est changé de *Élu* à *Prêt* ou *Bloqué*.

4.  **Sélection et restauration du contexte de P2** : L\'ordonnanceur sélectionne P2 dans la file des processus prêts. L\'OS charge l\'état du processeur à partir des informations stockées dans le PCB de P2.

5.  **Mise à jour du PCB de P2** : L\'état de P2 est changé de *Prêt* à *Élu*.

6.  L\'exécution de P2 reprend à l\'instruction pointée par son compteur de programme restauré.

La commutation de contexte est une opération coûteuse, une pure **surcharge** (*overhead*) pour le système, car pendant ce temps, aucun travail utile n\'est accompli du point de vue des applications. Les coûts peuvent être décomposés en deux catégories :

- **Coûts directs** : Le temps CPU consommé par le code de l\'ordonnanceur lui-même pour sauvegarder et restaurer les registres. C\'est la partie la plus visible du coût.

- **Coûts indirects** : Ces coûts, souvent bien plus significatifs, sont liés à la \"pollution\" des caches du processeur. Les processeurs modernes dépendent de multiples niveaux de caches (L1, L2, L3) pour leurs performances. Ces caches stockent les données et instructions récemment utilisées. Lorsqu\'une commutation de contexte se produit, le cache, qui était \"chaud\" (rempli de données pertinentes pour P1), devient \"froid\" pour P2. Le processus P2 subira donc une série de défauts de cache (\
  *cache misses*) au début de son exécution, car il devra aller chercher ses données dans la mémoire principale, beaucoup plus lente. De plus, le\
  **TLB** (*Translation Lookaside Buffer*), un cache spécialisé qui accélère la traduction des adresses virtuelles en adresses physiques, est souvent invalidé lors d\'un changement d\'espace d\'adressage, ce qui ralentit considérablement chaque accès mémoire initial. Cet impact invisible sur la hiérarchie mémoire est une contrainte fondamentale qui influence la conception des ordonnanceurs, qui cherchent à minimiser la fréquence des commutations de contexte.

## Vers le Parallélisme -- Les Threads

Le modèle de processus, avec son isolation mémoire stricte, est excellent pour la robustesse et la sécurité. Cependant, il présente des limitations en termes d\'efficacité pour les applications qui nécessitent un haut degré de parallélisme interne. La création d\'un processus et la communication entre processus sont des opérations coûteuses. Pour répondre à ce besoin, le concept de **thread** (ou fil d\'exécution) a été introduit, marquant un changement de paradigme majeur dans la conception des systèmes d\'exploitation.

### Le Processus \"Léger\" : Définition et Motivation

Un thread est l\'unité d\'exécution de base à laquelle le système d\'exploitation alloue du temps processeur. Alors qu\'un processus traditionnel est dit \"monothread\" (un seul fil d\'exécution), les applications modernes sont souvent \"multithreadées\", c\'est-à-dire qu\'un seul processus peut contenir plusieurs threads s\'exécutant simultanément ou en pseudo-parallèle.

Les threads sont souvent qualifiés de **processus légers** (*lightweight processes*) car leur création, leur terminaison et la commutation de contexte entre eux sont beaucoup plus rapides que pour les processus \"lourds\" traditionnels. Cette efficacité découle de leur principale caractéristique : le partage des ressources.

### Processus vs. Threads : Isolation contre Partage

La différence fondamentale entre un processus et un thread réside dans la gestion des ressources, en particulier la mémoire.

- **Processus** : Chaque processus possède son propre espace d\'adressage virtuel, complètement isolé des autres processus. Le noyau garantit cette isolation. La communication entre processus (IPC) est complexe et relativement lente, car elle doit transiter par le noyau pour traverser les frontières d\'adressage.

- **Threads** : Tous les threads au sein d\'un même processus **partagent** le même espace d\'adressage (les segments de code, de données et le tas) ainsi que d\'autres ressources comme les descripteurs de fichiers ouverts. Cependant, pour permettre une exécution indépendante, chaque thread possède son propre\
  **contexte d\'exécution**, qui inclut un compteur de programme, un jeu de registres et une pile d\'exécution unique.

Ce modèle de partage a des implications profondes. La communication entre threads est triviale et rapide : ils peuvent échanger des données simplement en lisant et en écrivant dans des variables partagées. En contrepartie, ce partage introduit des risques de **conditions de concurrence** (*race conditions*), où le résultat d\'une opération dépend de l\'ordonnancement imprévisible de plusieurs threads. Cela rend nécessaire l\'utilisation de mécanismes de **synchronisation** (tels que les mutex, les sémaphores ou les verrous) pour protéger l\'accès aux données partagées et garantir la cohérence.

  ------------------------------------- ------------------------------------------------------------ -----------------------------------------------------------------------
  Caractéristique                       Processus                                                    Thread

  **Poids**                             Lourd                                                        Léger

  **Mémoire**                           Espace d\'adressage isolé et protégé                         Espace d\'adressage partagé avec les autres threads du même processus

  **Coût de création/terminaison**      Élevé                                                        Faible

  **Coût de commutation de contexte**   Élevé (changement d\'espace d\'adressage, flush TLB)         Faible (pas de changement d\'espace d\'adressage)

  **Communication**                     Complexe et lente (IPC via le noyau)                         Simple et rapide (via la mémoire partagée)

  **Robustesse à la défaillance**       Élevée (un processus qui plante n\'affecte pas les autres)   Faible (un thread qui plante peut faire planter tout le processus)
  ------------------------------------- ------------------------------------------------------------ -----------------------------------------------------------------------

### Threads au Niveau Utilisateur vs. Threads au Niveau Noyau

Il existe deux principales manières d\'implémenter les threads, avec des compromis distincts en termes de performance et de fonctionnalité.

- **Threads au Niveau Utilisateur (User-Level Threads)** : Ces threads sont entièrement gérés par une bibliothèque logicielle dans l\'espace utilisateur (une *thread library*), sans aucune intervention ou connaissance du noyau. Pour le noyau, le processus entier apparaît comme une seule entité monothread.

  - **Avantages** : La création, la destruction et la commutation entre threads utilisateur sont extrêmement rapides, car elles se font par de simples appels de fonction au sein du processus, sans nécessiter de coûteux appels système. Cette approche est également très portable, car elle ne dépend pas de fonctionnalités spécifiques du noyau.

  - **Inconvénients** : Le modèle présente deux défauts majeurs. Premièrement, si un thread utilisateur effectue un appel système bloquant (par exemple, une lecture sur le disque), le noyau bloque le processus entier, y compris tous les autres threads qui auraient pu continuer à s\'exécuter. Deuxièmement, il est impossible de tirer parti des architectures multi-cœurs, car le noyau n\'a connaissance que d\'un seul thread d\'exécution et ne peut donc pas répartir les threads du processus sur plusieurs processeurs.

- **Threads au Niveau Noyau (Kernel-Level Threads)** : Dans ce modèle, les threads sont gérés directement par le système d\'exploitation. Le noyau a connaissance de chaque thread, maintient un TCB (\
  *Thread Control Block*) pour chacun et les ordonnance de manière indépendante.

  - **Avantages** : Ce modèle résout les problèmes des threads utilisateur. Si un thread se bloque, le noyau peut simplement ordonnancer un autre thread du même processus. Surtout, il permet un\
    **vrai parallélisme** : le noyau peut exécuter plusieurs threads d\'un même processus simultanément sur différents cœurs de processeur.

  - **Inconvénients** : La gestion des threads noyau est plus lente. Chaque opération (création, synchronisation) requiert un appel système et une transition en mode noyau, ce qui induit une surcharge plus importante que pour les threads utilisateur.

  ---------------------------------------- ------------------------------------ ----------------------------------
  Caractéristique                          Threads Utilisateur                  Threads Noyau

  **Gestion**                              Bibliothèque en espace utilisateur   Noyau du système d\'exploitation

  **Performance (Création/Commutation)**   Très rapide (appel de fonction)      Plus lente (appel système)

  **Gestion des appels bloquants**         Bloque tout le processus             Ne bloque que le thread concerné

  **Parallélisme multi-cœur**              Impossible                           Possible et efficace

  **Portabilité**                          Élevée (indépendant de l\'OS)        Faible (dépendant de l\'OS)
  ---------------------------------------- ------------------------------------ ----------------------------------

### Modèles de Mappage des Threads

La relation entre les threads utilisateur et les threads noyau est définie par un modèle de mappage.

- **Modèle Many-to-One** : Plusieurs threads utilisateur sont mappés sur un unique thread noyau. C\'est le modèle correspondant à l\'implémentation pure des threads utilisateur. Il est efficace en termes de gestion mais souffre des problèmes de blocage et de l\'incapacité à exploiter le multi-cœur.

- **Modèle One-to-One** : Chaque thread utilisateur est associé à un thread noyau dédié. C\'est le modèle le plus courant dans les systèmes d\'exploitation modernes comme Linux et Windows. Il offre un parallélisme maximal et une gestion robuste des appels bloquants, au prix d\'une surcharge potentielle si un très grand nombre de threads est créé, car chaque thread utilisateur consomme des ressources noyau.

- **Modèle Many-to-Many** : Ce modèle, aussi appelé modèle à deux niveaux, multiplexe un grand nombre de threads utilisateur sur un plus petit nombre de threads noyau. Il tente de combiner les avantages des deux autres approches : la flexibilité de créer de nombreux threads légers en espace utilisateur, tout en permettant au noyau de paralléliser l\'exécution sur plusieurs cœurs. Ce modèle est cependant nettement plus complexe à implémenter et à gérer.

L\'introduction des threads a représenté une évolution fondamentale, dissociant le concept de \"processus\" en deux entités distinctes : le processus en tant que conteneur de ressources et le thread en tant qu\'unité d\'exécution. Cette séparation a permis un parallélisme à grain fin, essentiel pour exploiter efficacement l\'avènement des processeurs multi-cœurs, une tâche pour laquelle le modèle de processus traditionnel et les threads purement utilisateur étaient mal adaptés.

## L\'Arbitrage du Temps -- L\'Ordonnancement du Processeur

Dans un système multiprogrammé, plusieurs processus peuvent se trouver simultanément à l\'état \"Prêt\", en compétition pour l\'accès à la ressource la plus critique : le processeur. Le module du noyau responsable de décider quel processus exécuter et pour combien de temps est l\'**ordonnanceur** (*scheduler*). L\'ordonnancement est une fonction centrale qui a un impact direct sur la performance et la réactivité perçue du système.

### Objectifs et Critères de l\'Ordonnancement

La conception d\'un ordonnanceur est un exercice de compromis, car il doit tenter de satisfaire plusieurs objectifs souvent contradictoires. Les principaux critères de performance sont :

- **Utilisation du CPU** : Maintenir le processeur aussi occupé que possible.

- **Débit (Throughput)** : Maximiser le nombre de processus terminés par unité de temps.

- **Temps de rotation (Turnaround Time)** : Minimiser le temps total qui s\'écoule entre la soumission d\'un processus et sa terminaison.

- **Temps d\'attente (Waiting Time)** : Minimiser le temps total qu\'un processus passe dans la file d\'attente des processus prêts.

- **Temps de réponse (Response Time)** : Dans les systèmes interactifs, minimiser le temps entre une action de l\'utilisateur et la première réponse visible du système.

- **Équité (Fairness)** : S\'assurer que chaque processus obtient une part équitable du temps CPU, afin d\'éviter le phénomène de **famine** (*starvation*), où un processus pourrait être indéfiniment ignoré par l\'ordonnanceur.

Le choix d\'un algorithme d\'ordonnancement n\'est pas une décision purement technique ; il s\'agit d\'un choix de politique qui reflète la finalité du système. Un système de calcul par lots (batch) privilégiera le débit au détriment du temps de réponse, tandis qu\'un système de bureau interactif fera le contraire. Un système temps réel, quant à lui, aura pour objectif principal le respect absolu des échéances temporelles. Il n\'existe donc pas d\'algorithme universellement \"meilleur\", mais plutôt des algorithmes adaptés à des contextes d\'utilisation spécifiques.

### Ordonnancement Préemptif vs. Non-Préemptif

Les politiques d\'ordonnancement se divisent en deux grandes catégories  :

- **Non-préemptif (ou coopératif)** : Une fois qu\'un processus a été élu, il conserve le contrôle du CPU jusqu\'à ce qu\'il le libère volontairement, soit en terminant son exécution, soit en se bloquant pour une opération d\'E/S. L\'ordonnanceur n\'a pas le pouvoir de lui retirer le processeur de force.

- **Préemptif** : L\'ordonnanceur peut interrompre un processus en cours d\'exécution, même contre son gré, pour allouer le CPU à un autre processus. Cette préemption se produit généralement à la suite d\'une interruption d\'horloge (le \"quantum\" de temps du processus est écoulé) ou à l\'arrivée d\'un processus de plus haute priorité. La quasi-totalité des systèmes d\'exploitation modernes utilise un ordonnancement préemptif.

### Analyse des Algorithmes d\'Ordonnancement

Plusieurs algorithmes classiques ont été développés pour mettre en œuvre ces politiques.

- **Premier Arrivé, Premier Servi (FCFS / FIFO)** : Cet algorithme non-préemptif est le plus simple. Les processus sont placés dans une file d\'attente et sont servis dans leur ordre d\'arrivée. Bien que simple et équitable au premier abord, il peut être très inefficace. Si un processus long arrive juste avant plusieurs processus courts, ces derniers devront attendre très longtemps, dégradant fortement le temps d\'attente moyen. C\'est ce qu\'on appelle l\'\
  **effet de convoi**.

- **Plus Court d\'Abord (SJF - Shortest Job First)** : Cet algorithme, qui peut être préemptif ou non, sélectionne le processus dont la prochaine rafale d\'utilisation du CPU (*CPU burst*) est la plus courte. Il est prouvé que SJF est optimal pour minimiser le temps d\'attente moyen. Son principal défaut est son caractère irréalisable en pratique, car il est impossible de connaître à l\'avance la durée exacte du prochain\
  *CPU burst*. De plus, il peut provoquer la famine des processus longs, qui risquent de ne jamais être exécutés si des processus courts arrivent continuellement.

- **Tourniquet (Round-Robin - RR)** : C\'est un algorithme préemptif conçu pour les systèmes à temps partagé. La file des processus prêts est gérée comme une file circulaire. Chaque processus obtient le CPU pour une petite durée fixe appelée\
  **quantum de temps** (typiquement 10-100 ms). Si le processus est toujours en cours à la fin de son quantum, il est préempté et replacé à la fin de la file d\'attente. Cet algorithme est très équitable et offre d\'excellents temps de réponse, mais sa performance est très sensible à la taille du quantum : un quantum trop grand le fait converger vers FCFS, tandis qu\'un quantum trop petit augmente la surcharge due aux commutations de contexte fréquentes.

- **Ordonnancement par Priorités** : À chaque processus est associée une valeur de priorité. L\'ordonnanceur alloue le CPU au processus prêt ayant la plus haute priorité. Cet algorithme peut être préemptif (si un processus de haute priorité arrive, il préempte le processus en cours de plus faible priorité) ou non-préemptif. Le risque majeur est la famine des processus à faible priorité. Pour y remédier, des mécanismes de\
  **vieillissement** (*aging*) sont souvent mis en œuvre, où la priorité d\'un processus augmente progressivement tant qu\'il reste dans la file d\'attente.

### Analyse Comparative avec Diagrammes de Gantt

Pour visualiser et comparer les performances de ces algorithmes, on utilise couramment le **diagramme de Gantt**. C\'est un diagramme à barres qui illustre l\'allocation du processeur aux différents processus au fil du temps.

Considérons l\'ensemble de processus suivant, tous arrivant à l\'instant t=0 :

  ------------------------------ ---------------------------------
  Processus                      Durée d\'exécution (Burst Time)

  P1                             24

  P2                             3

  P3                             3
  ------------------------------ ---------------------------------

- Avec FCFS (ordre d\'arrivée P1, P2, P3) :\
  Le diagramme de Gantt est :\
  !(https://i.imgur.com/g8uX3Yl.png)\
  Le temps d\'attente moyen est de (0+24+27)/3=17 ms.81

- Avec SJF :\
  L\'ordonnanceur choisit P2 (ou P3), puis l\'autre, et enfin P1.\
  Le diagramme de Gantt est :\
  !(https://i.imgur.com/Tq9Yj9I.png)\
  Le temps d\'attente moyen est de (6+0+3)/3=3 ms.81

- Avec Round-Robin (quantum = 4 ms) :\
  L\'exécution alterne entre les processus.\
  Le diagramme de Gantt est :\
  !(https://i.imgur.com/k9bJz2n.png)\
  Le temps d\'attente moyen est de ((10−4)+(4−0)+(7−0))/3=(6+4+7)/3≈5.67 ms.

Cet exemple simple illustre de manière frappante comment le choix de l\'algorithme d\'ordonnancement a un impact majeur sur les performances du système, en particulier sur le temps d\'attente perçu par les processus.

## Conclusion : Synthèse des Compromis et Vision d\'Ensemble

Ce chapitre a exploré les concepts fondamentaux qui sous-tendent le fonctionnement de tout système d\'exploitation moderne. De l\'architecture du noyau à la gestion des processus et à l\'ordonnancement du temps processeur, un thème récurrent émerge : le système d\'exploitation est un art du compromis.

Nous avons vu comment l\'OS agit simultanément comme une **machine étendue**, offrant des abstractions simples pour masquer la complexité du matériel, et comme un **gestionnaire de ressources**, arbitrant l\'accès au matériel de manière contrôlée et sécurisée. Cette dualité est rendue possible par la séparation matérielle entre le **mode noyau** et le **mode utilisateur**, une frontière franchie de manière contrôlée par le mécanisme des **appels système**.

L\'analyse des architectures de noyau a révélé un arbitrage fondamental entre la performance brute des **noyaux monolithiques** et la robustesse modulaire des **micronoyaux**, un débat qui a conduit à la convergence vers des solutions **hybrides** et modulaires dans les systèmes contemporains.

Le **processus**, en tant qu\'instance d\'un programme en exécution, a été présenté comme l\'abstraction centrale de la gestion des tâches. Son identité est encapsulée dans le **Bloc de Contrôle du Processus (PCB)**, et sa vie est rythmée par des transitions entre des états bien définis. L\'introduction des **threads** a affiné ce modèle en séparant l\'unité d\'exécution de l\'unité de ressource, permettant un parallélisme à grain fin essentiel aux architectures multi-cœurs, mais introduisant de nouveaux défis de synchronisation. Cet arbitrage entre l\'isolation sécurisée des processus et l\'efficacité de la communication des threads est au cœur de la conception des applications concurrentes.

Enfin, l\'**ordonnancement** du processeur a été montré non pas comme un simple problème algorithmique, mais comme une décision de politique qui définit le caractère du système. L\'équilibre entre le débit, le temps de réponse et l\'équité est un choix qui dépend entièrement de l\'usage prévu du système, qu\'il soit destiné au calcul intensif, à l\'interaction utilisateur ou à des applications temps réel critiques.

En définitive, chaque concept étudié --- abstraction, protection, processus, thread, ordonnancement --- est une pièce d\'un puzzle complexe. Ces pièces sont interdépendantes : la protection rend nécessaires les appels système, qui permettent au noyau de gérer les processus, qui sont eux-mêmes composés de threads, lesquels sont ordonnancés pour partager le temps processeur. Comprendre ces concepts et les compromis inhérents à chacun est la clé pour appréhender la complexité, la puissance et l\'élégance des systèmes d\'exploitation qui animent le monde numérique.

